<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>chemgraphbuilder API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>chemgraphbuilder</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="chemgraphbuilder.add_graph_nodes" href="add_graph_nodes.html">chemgraphbuilder.add_graph_nodes</a></code></dt>
<dd>
<div class="desc"><p>Module for adding node data from CSV files to a Neo4j database …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.add_graph_relationships" href="add_graph_relationships.html">chemgraphbuilder.add_graph_relationships</a></code></dt>
<dd>
<div class="desc"><p>Module for adding relationship data from CSV files to a Neo4j database …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.graph_nodes_loader" href="graph_nodes_loader.html">chemgraphbuilder.graph_nodes_loader</a></code></dt>
<dd>
<div class="desc"><p>Module to load data into a Neo4j graph database for different node types …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.graph_relationships_loader" href="graph_relationships_loader.html">chemgraphbuilder.graph_relationships_loader</a></code></dt>
<dd>
<div class="desc"><p>GraphRelationshipsLoader class for loading graph relationships into a Neo4j database.</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.neo4jdriver" href="neo4jdriver.html">chemgraphbuilder.neo4jdriver</a></code></dt>
<dd>
<div class="desc"><p>Module for managing connections to a Neo4j database …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.node_collector_processor" href="node_collector_processor.html">chemgraphbuilder.node_collector_processor</a></code></dt>
<dd>
<div class="desc"><p>NodesCollectorProcessor Module …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.node_data_processor" href="node_data_processor.html">chemgraphbuilder.node_data_processor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.node_properties_extractor" href="node_properties_extractor.html">chemgraphbuilder.node_properties_extractor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.relationship_collector_processor" href="relationship_collector_processor.html">chemgraphbuilder.relationship_collector_processor</a></code></dt>
<dd>
<div class="desc"><p>Module to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and …</p></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.relationship_data_processor" href="relationship_data_processor.html">chemgraphbuilder.relationship_data_processor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.relationship_properties_extractor" href="relationship_properties_extractor.html">chemgraphbuilder.relationship_properties_extractor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="chemgraphbuilder.setup_data_folder" href="setup_data_folder.html">chemgraphbuilder.setup_data_folder</a></code></dt>
<dd>
<div class="desc"><p>Module to set up a data directory with a predefined structure …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="chemgraphbuilder.AddGraphNodes"><code class="flex name class">
<span>class <span class="ident">AddGraphNodes</span></span>
<span>(</span><span>driver)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used to add node data from a CSV file or a directory of CSV files to a Neo4j database.</p>
<h2 id="methods">Methods:</h2>
<p>create_uniqueness_constraint(driver, label, unique_property):
Create a uniqueness constraint for the unique property of nodes in Neo4j.
generate_cypher_queries(node_dict, label, unique_property):
Generate Cypher queries to update nodes in Neo4j based on the data from the CSV file.
execute_queries(queries):
Execute a list of provided Cypher queries against the Neo4j database.
read_csv_file(file_path, unique_property):
Read data from a CSV file and extract node properties.
combine_csv_files(input_directory):
Combine multiple CSV files with the same columns into a single DataFrame.
process_and_add_nodes(file_path, label, unique_property):
Process the CSV file and add node data to the Neo4j database.
process_and_add_nodes_from_directory(directory_path, label, unique_property):
Combine CSV files from a directory and add node data to the Neo4j database.</p>
<p>Initializes the AddGraphNodes class with a Neo4j driver.</p>
<h2 id="parameters">Parameters:</h2>
<p>driver : neo4j.GraphDatabase.driver
A driver instance to connect to the Neo4j database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AddGraphNodes(Neo4jBase):
    &#34;&#34;&#34;
    A class used to add node data from a CSV file or a directory of CSV files to a Neo4j database.

    Methods:
    --------
    create_uniqueness_constraint(driver, label, unique_property):
        Create a uniqueness constraint for the unique property of nodes in Neo4j.
    generate_cypher_queries(node_dict, label, unique_property):
        Generate Cypher queries to update nodes in Neo4j based on the data from the CSV file.
    execute_queries(queries):
        Execute a list of provided Cypher queries against the Neo4j database.
    read_csv_file(file_path, unique_property):
        Read data from a CSV file and extract node properties.
    combine_csv_files(input_directory):
        Combine multiple CSV files with the same columns into a single DataFrame.
    process_and_add_nodes(file_path, label, unique_property):
        Process the CSV file and add node data to the Neo4j database.
    process_and_add_nodes_from_directory(directory_path, label, unique_property):
        Combine CSV files from a directory and add node data to the Neo4j database.
    &#34;&#34;&#34;

    def __init__(self, driver):
        &#34;&#34;&#34;
        Initializes the AddGraphNodes class with a Neo4j driver.

        Parameters:
        -----------
        driver : neo4j.GraphDatabase.driver
            A driver instance to connect to the Neo4j database.
        &#34;&#34;&#34;
        super().__init__()
        self.driver = driver
        self.logger.info(&#34;AddGraphNodes class initialized.&#34;)

    @staticmethod
    def create_uniqueness_constraint(driver, label, unique_property):
        &#34;&#34;&#34;
        Create a uniqueness constraint for the unique property of nodes in Neo4j.

        Parameters:
        -----------
        driver : neo4j.GraphDatabase.driver
            A driver instance to connect to the Neo4j database.
        label : str
            The label of the node.
        unique_property : str
            The unique property of the node.
        &#34;&#34;&#34;
        constraint_query = (
            f&#34;CREATE CONSTRAINT IF NOT EXISTS FOR (n:{label}) &#34;
            f&#34;REQUIRE n.{unique_property} IS UNIQUE&#34;
        )
        with driver.session() as session:
            try:
                session.run(constraint_query)
                logging.info(
                    &#34;Uniqueness constraint created successfully on %s property of %s nodes.&#34;,
                    unique_property, label)
            except Exception as e:
                logging.error(&#34;Failed to create uniqueness constraint: %s&#34;, e)

    @staticmethod
    def _generate_property_string(value):
        if isinstance(value, (int, float)):
            return value
        try:
            return float(value)
        except (TypeError, ValueError):
            escaped_value = &#34;&#39;&#34; + str(value).replace(&#34;&#39;&#34;, &#34;\\&#39;&#34;).replace(&#34;\n&#34;, &#34;\\n&#34;) + &#34;&#39;&#34;
            return escaped_value

    def generate_cypher_queries(self, node_dict, label, unique_property):
        &#34;&#34;&#34;
        Generate Cypher queries for updating Neo4j based on the provided node data dictionary.

        Parameters:
        -----------
        node_dict : dict
            A dictionary with unique identifiers as keys and node data as values.
        label : str
            The label of the node.
        unique_property : str
            The unique property of the node.

        Yields:
        -------
        str
            A Cypher query string.
        &#34;&#34;&#34;
        # Create an index for the unique_property
        create_index_query = f&#34;CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON (n.{unique_property})&#34;
        self.logger.debug(create_index_query)
        yield create_index_query

        for unique_id, properties in node_dict.items():
            unique_id = f&#39;&#34;{unique_id}&#34;&#39; if isinstance(unique_id, str) else unique_id
            query = f&#34;MERGE (n:{label} {{{unique_property}: {unique_id}}})&#34;
            set_clauses = [
                f&#34;n.{prop.replace(&#39; &#39;, &#39;&#39;)} = {self._generate_property_string(value)}&#34;
                for prop, value in properties.items()
            ]
            if set_clauses:
                query += &#34; SET &#34; + &#34;, &#34;.join(set_clauses)
            else:
                query += &#34;;&#34;
            self.logger.debug(query)
            yield query
        self.logger.info(&#34;Cypher queries generated successfully.&#34;)

    def execute_queries(self, queries):
        &#34;&#34;&#34;
        Execute the provided list of Cypher queries against the Neo4j database.

        Parameters:
        -----------
        queries : list
            A list of Cypher query strings to execute.
        &#34;&#34;&#34;
        self.logger.info(&#34;Executing Cypher queries...&#34;)
        with self.driver.session() as session:
            self.logger.info(&#34;Executing Cypher queries Started....&#34;)
            for query in queries:
                try:
                    session.run(query)
                except Exception as e:
                    self.logger.error(&#34;Failed to execute query: %s&#34;, e)
        self.logger.info(&#34;All queries executed.&#34;)

    def read_csv_file(self, file_path, unique_property):
        &#34;&#34;&#34;
        Read data from a CSV file and extract node properties.

        Parameters:
        -----------
        file_path : str
            The path to the CSV file.
        unique_property : str
            The column name that serves as the unique identifier for the nodes.

        Returns:
        --------
        dict
            A dictionary with unique identifiers as keys and extracted data as values.
        &#34;&#34;&#34;
        self.logger.info(&#34;Reading data from CSV file: %s&#34;, file_path)
        df = pd.read_csv(file_path).dropna(subset=[unique_property], how=&#39;any&#39;)
        node_dict = {
            row[unique_property]: row.drop(labels=[unique_property]).to_dict()
            for _, row in df.iterrows()
        }
        self.logger.info(&#34;Successfully read data for %d nodes from CSV.&#34;, len(node_dict))
        return node_dict

    def combine_csv_files(self, input_directory):
        &#34;&#34;&#34;
        Combine multiple CSV files with the same columns into a single DataFrame.

        Parameters:
        -----------
        input_directory : str
            The directory containing the CSV files to be combined.

        Returns:
        --------
        DataFrame
            A combined DataFrame containing data from all the CSV files.
        &#34;&#34;&#34;
        self.logger.info(&#34;Combining CSV files from directory: %s&#34;, input_directory)
        dfs = [
            pd.read_csv(os.path.join(input_directory, file))
            for file in os.listdir(input_directory)
            if file.endswith(&#34;.csv&#34;)
        ]
        combined_df = pd.concat(dfs, ignore_index=True)
        self.logger.info(&#34;Successfully combined %d CSV files.&#34;, len(dfs))
        return combined_df

    def process_and_add_nodes(self, file_path, label, unique_property):
        &#34;&#34;&#34;
        Process the CSV file and add node data to the Neo4j database.

        Parameters:
        -----------
        file_path : str
            The path to the CSV file.
        label : str
            The label of the node.
        unique_property : str
            The unique property of the node.
        &#34;&#34;&#34;
        self.logger.info(&#34;Processing and adding nodes from file: %s&#34;, file_path)
        node_dict = self.read_csv_file(file_path, unique_property)
        queries = list(self.generate_cypher_queries(node_dict, label, unique_property))
        self.execute_queries(queries)
        self.logger.info(&#34;Successfully processed and added nodes from file: %s&#34;, file_path)

    def process_and_add_nodes_from_directory(self, directory_path, label, unique_property):
        &#34;&#34;&#34;
        Combine CSV files from a directory and add node data to the Neo4j database.

        Parameters:
        -----------
        directory_path : str
            The path to the directory containing the CSV files.
        label : str
            The label of the node.
        unique_property : str
            The unique property of the node.
        &#34;&#34;&#34;
        self.logger.info(&#34;Processing and adding nodes from directory: %s&#34;, directory_path)
        combined_df = self.combine_csv_files(directory_path)
        temp_file = os.path.join(directory_path, &#34;combined_temp.csv&#34;)
        combined_df.to_csv(temp_file, index=False)
        self.process_and_add_nodes(temp_file, label, unique_property)
        os.remove(temp_file)
        self.logger.info(&#34;Successfully processed and added nodes from directory: %s&#34;,
                         directory_path)

    def public_generate_property_string(self, value):
        &#34;&#34;&#34;
        Public method to access the protected _generate_property_string method for testing.

        Parameters:
        -----------
        value : Any
            The value to be formatted.

        Returns:
        --------
        str
            The formatted property string.
        &#34;&#34;&#34;
        return self._generate_property_string(value)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="chemgraphbuilder.neo4jdriver.Neo4jBase" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase">Neo4jBase</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="chemgraphbuilder.AddGraphNodes.create_uniqueness_constraint"><code class="name flex">
<span>def <span class="ident">create_uniqueness_constraint</span></span>(<span>driver, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a uniqueness constraint for the unique property of nodes in Neo4j.</p>
<h2 id="parameters">Parameters:</h2>
<p>driver : neo4j.GraphDatabase.driver
A driver instance to connect to the Neo4j database.
label : str
The label of the node.
unique_property : str
The unique property of the node.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.AddGraphNodes.combine_csv_files"><code class="name flex">
<span>def <span class="ident">combine_csv_files</span></span>(<span>self, input_directory)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine multiple CSV files with the same columns into a single DataFrame.</p>
<h2 id="parameters">Parameters:</h2>
<p>input_directory : str
The directory containing the CSV files to be combined.</p>
<h2 id="returns">Returns:</h2>
<p>DataFrame
A combined DataFrame containing data from all the CSV files.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.execute_queries"><code class="name flex">
<span>def <span class="ident">execute_queries</span></span>(<span>self, queries)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute the provided list of Cypher queries against the Neo4j database.</p>
<h2 id="parameters">Parameters:</h2>
<p>queries : list
A list of Cypher query strings to execute.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.generate_cypher_queries"><code class="name flex">
<span>def <span class="ident">generate_cypher_queries</span></span>(<span>self, node_dict, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Cypher queries for updating Neo4j based on the provided node data dictionary.</p>
<h2 id="parameters">Parameters:</h2>
<p>node_dict : dict
A dictionary with unique identifiers as keys and node data as values.
label : str
The label of the node.
unique_property : str
The unique property of the node.</p>
<h2 id="yields">Yields:</h2>
<p>str
A Cypher query string.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.process_and_add_nodes"><code class="name flex">
<span>def <span class="ident">process_and_add_nodes</span></span>(<span>self, file_path, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Process the CSV file and add node data to the Neo4j database.</p>
<h2 id="parameters">Parameters:</h2>
<p>file_path : str
The path to the CSV file.
label : str
The label of the node.
unique_property : str
The unique property of the node.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.process_and_add_nodes_from_directory"><code class="name flex">
<span>def <span class="ident">process_and_add_nodes_from_directory</span></span>(<span>self, directory_path, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine CSV files from a directory and add node data to the Neo4j database.</p>
<h2 id="parameters">Parameters:</h2>
<p>directory_path : str
The path to the directory containing the CSV files.
label : str
The label of the node.
unique_property : str
The unique property of the node.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.public_generate_property_string"><code class="name flex">
<span>def <span class="ident">public_generate_property_string</span></span>(<span>self, value)</span>
</code></dt>
<dd>
<div class="desc"><p>Public method to access the protected _generate_property_string method for testing.</p>
<h2 id="parameters">Parameters:</h2>
<p>value : Any
The value to be formatted.</p>
<h2 id="returns">Returns:</h2>
<p>str
The formatted property string.</p></div>
</dd>
<dt id="chemgraphbuilder.AddGraphNodes.read_csv_file"><code class="name flex">
<span>def <span class="ident">read_csv_file</span></span>(<span>self, file_path, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a CSV file and extract node properties.</p>
<h2 id="parameters">Parameters:</h2>
<p>file_path : str
The path to the CSV file.
unique_property : str
The column name that serves as the unique identifier for the nodes.</p>
<h2 id="returns">Returns:</h2>
<p>dict
A dictionary with unique identifiers as keys and extracted data as values.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="chemgraphbuilder.neo4jdriver.Neo4jBase" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase">Neo4jBase</a></b></code>:
<ul class="hlist">
<li><code><a title="chemgraphbuilder.neo4jdriver.Neo4jBase.close" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase.close">close</a></code></li>
<li><code><a title="chemgraphbuilder.neo4jdriver.Neo4jBase.connect_to_neo4j" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase.connect_to_neo4j">connect_to_neo4j</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships"><code class="flex name class">
<span>class <span class="ident">AddGraphRelationships</span></span>
<span>(</span><span>driver)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used to add relationship data from a CSV file or a directory of CSV
files to a Neo4j database.</p>
<h2 id="methods">Methods</h2>
<p>generate_cypher_queries_from_file(file_path, rel_type, source_label, destination_label, rel_type_column=None):
Generate Cypher queries to create relationships in Neo4j based on the data from the CSV file.
generate_cypher_queries_from_directories(directory, rel_type, source_label, destination_label, rel_type_column=None):
Generate Cypher queries to create relationships in Neo4j by merging CSV files from a directory.
execute_queries(queries, batch_size=100):
Execute a list of provided Cypher queries against the Neo4j database.
combine_csv_files(input_directory):
Combine multiple CSV files with the same columns into a single DataFrame.
process_and_add_relationships(file_path, rel_type, source_label, destination_label, rel_type_column=None):
Process the CSV file and add relationship data to the Neo4j database.
process_and_add_relationships_from_directory(directory_path, rel_type, source_label, destination_label, rel_type_column=None):
Combine CSV files from a directory and add relationship data to the Neo4j database.</p>
<p>Initializes the AddGraphRelationships class with a Neo4j driver.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>driver</code></strong> :&ensp;<code>neo4j.GraphDatabase.driver</code></dt>
<dd>A driver instance to connect to the Neo4j database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AddGraphRelationships(Neo4jBase):
    &#34;&#34;&#34;
    A class used to add relationship data from a CSV file or a directory of CSV
    files to a Neo4j database.

    Methods
    -------
    generate_cypher_queries_from_file(file_path, rel_type, source_label, destination_label, rel_type_column=None):
        Generate Cypher queries to create relationships in Neo4j based on the data from the CSV file.
    generate_cypher_queries_from_directories(directory, rel_type, source_label, destination_label, rel_type_column=None):
        Generate Cypher queries to create relationships in Neo4j by merging CSV files from a directory.
    execute_queries(queries, batch_size=100):
        Execute a list of provided Cypher queries against the Neo4j database.
    combine_csv_files(input_directory):
        Combine multiple CSV files with the same columns into a single DataFrame.
    process_and_add_relationships(file_path, rel_type, source_label, destination_label, rel_type_column=None):
        Process the CSV file and add relationship data to the Neo4j database.
    process_and_add_relationships_from_directory(directory_path, rel_type, source_label, destination_label, rel_type_column=None):
        Combine CSV files from a directory and add relationship data to the Neo4j database.
    &#34;&#34;&#34;

    def __init__(self, driver):
        &#34;&#34;&#34;
        Initializes the AddGraphRelationships class with a Neo4j driver.

        Parameters
        ----------
        driver : neo4j.GraphDatabase.driver
            A driver instance to connect to the Neo4j database.
        &#34;&#34;&#34;
        super().__init__()
        self.driver = driver
        self.logger.info(&#34;AddGraphRelationships class initialized.&#34;)

    @staticmethod
    def _generate_property_string(value):
        &#34;&#34;&#34;
        Generate a property string for Cypher queries.

        Parameters
        ----------
        value : any
            The value to be converted to a string.

        Returns
        -------
        str
            The formatted string for the Cypher query.
        &#34;&#34;&#34;
        if isinstance(value, (int, float)):
            return value
        try:
            evaluated_value = ast.literal_eval(value)
            if isinstance(evaluated_value, (int, float)):
                return evaluated_value
            if isinstance(evaluated_value, dict):
                json_str = json.dumps(evaluated_value).replace(&#39;&#34;&#39;, &#39;\\&#34;&#39;).replace(&#34;&#39;&#34;, &#34;\\&#39;&#34;)
                return f&#34;&#39;{json_str}&#39;&#34;
        except (ValueError, SyntaxError):
            pass

        escaped_value = &#34;&#39;&#34; + value.replace(&#34;&#39;&#34;, &#34;\\&#39;&#34;).replace(&#34;\n&#34;, &#34;\\n&#34;) + &#34;&#39;&#34;
        return escaped_value

    def _process_properties(self, row, source_column, destination_column,
                            rel_type_column, standard_id):
        &#34;&#34;&#34;
        Process the properties for the relationship from the CSV row.

        Parameters
        ----------
        row : pandas.Series
            The row from the DataFrame.
        source_column : str
            The column name for the source node.
        destination_column : str
            The column name for the destination node.
        rel_type_column : str
            The column name for the relationship type, if it is to be extracted from the CSV file.
        standard_id : dict
            A dictionary mapping standard IDs.

        Returns
        -------
        dict
            A dictionary of properties for the relationship.
        &#34;&#34;&#34;
        columns_to_drop = [source_column, destination_column]
        if rel_type_column:
            columns_to_drop.append(rel_type_column)
        properties = row.drop(labels=columns_to_drop).to_dict()
        properties = {k: v for k, v in properties.items() if not pd.isna(v)}
        properties = {standard_id.get(k, k): v for k, v in properties.items()}
        return properties

    def _generate_query(self, source, target, relationship_type, properties,
                        source_label, destination_label, standard_id,
                        source_column, destination_column):
        &#34;&#34;&#34;
        Generate a Cypher query for creating a relationship in Neo4j.

        Parameters
        ----------
        source : any
            The source node identifier.
        target : any
            The target node identifier.
        relationship_type : str
            The type of the relationship.
        properties : dict
            The properties of the relationship.
        source_label : str
            The label for the source node.
        destination_label : str
            The label for the destination node.
        standard_id : dict
            A dictionary mapping standard IDs.
        source_column : str
            The column name for the source node.
        destination_column : str
            The column name for the destination node.

        Returns
        -------
        str
            A Cypher query string.
        &#34;&#34;&#34;
        source_value = self._generate_property_string(source)
        target_value = self._generate_property_string(target)

        query = (
            f&#34;MATCH (a:{source_label} {{{standard_id[source_column]}: {source_value}}}), &#34;
            f&#34;(b:{destination_label} {{{standard_id[destination_column]}: {target_value}}}) &#34;
            f&#34;MERGE (a)-[r:{relationship_type}]-&gt;(b)&#34;
        )

        if properties:
            set_clauses = [
                f&#34;r.{prop.replace(&#39; &#39;, &#39;&#39;).replace(&#39;[&#39;, &#39;_&#39;).replace(&#39;]&#39;, &#39;_&#39;)}&#34;
                f&#34;= {self._generate_property_string(value)}&#34;
                for prop, value in properties.items()
            ]
            if set_clauses:
                query += &#34; SET &#34; + &#34;, &#34;.join(set_clauses)

        self.logger.debug(f&#34;Generated query: {query}&#34;)
        return query

    def generate_cypher_queries_from_file(self, file_path, rel_type, source_label,
                                          destination_label, rel_type_column=None):
        &#34;&#34;&#34;
        Generate Cypher queries for creating relationships in Neo4j based on the provided CSV file.

        Parameters
        ----------
        file_path : str
            The path to the CSV file.
        rel_type : str
            The type of the relationship.
        source_label : str
            The label for the source node.
        destination_label : str
            The label for the destination node.
        rel_type_column : str, optional
            The column name for the relationship type, if it is to be extracted from the CSV file.

        Yields
        ------
        str
            A Cypher query string.
        &#34;&#34;&#34;
        standard_id = {
            &#39;GeneID&#39;: &#39;GeneID&#39;,
            &#39;Gene ID&#39;: &#39;GeneID&#39;,
            &#39;Target GeneID&#39;: &#39;GeneID&#39;,
            &#39;geneids&#39;: &#39;GeneID&#39;,
            &#39;AssayID&#39;: &#39;AssayID&#39;,
            &#39;Assay ID&#39;: &#39;AssayID&#39;,
            &#39;AID&#39;: &#39;AssayID&#39;,
            &#39;ID_1&#39;: &#39;CompoundID&#39;,
            &#39;ID_2&#39;: &#39;CompoundID&#39;,
            &#39;substratecid&#39;: &#39;CompoundID&#39;,
            &#39;metabolitecid&#39;: &#39;CompoundID&#39;,
            &#39;Compound ID&#39;: &#39;CompoundID&#39;,
            &#39;CompoundID&#39;: &#39;CompoundID&#39;,
            &#39;CID&#39;: &#39;CompoundID&#39;,
            &#39;Similar CIDs&#39;: &#39;CompoundID&#39;,
            &#39;Target Accession&#39;: &#39;ProteinRefSeqAccession&#39;
        }
        self.logger.info(f&#34;Reading data from CSV file: {file_path}&#34;)
        df = pd.read_csv(file_path)
        source_column, destination_column = df.columns[:2]
        df = df.dropna(subset=[source_column, destination_column], how=&#39;any&#39;)
        if df.empty:
            self.logger.error(&#34;The CSV file %s is empty or contains no valid data.&#34;,
                              file_path)
            return
        if rel_type_column:
            df = df.dropna(subset=[rel_type_column])

        for _, row in df.iterrows():
            source = row[source_column]
            relationship_type = row[rel_type_column] if rel_type_column else rel_type
            properties = self._process_properties(row, source_column,
                                                  destination_column,
                                                  rel_type_column, standard_id)

            if rel_type == &#39;IS_SIMILAR_TO&#39;:
                targets = ast.literal_eval(row[destination_column])
                for target in targets:
                    query = self._generate_query(
                        source, target, relationship_type, properties,
                        source_label,destination_label, standard_id,
                        source_column, destination_column
                    )
                    yield query
            elif rel_type == &#39;CO_OCCURS_IN_LITERATURE&#39;:
                source = ast.literal_eval(row[source_column])
                if isinstance(source, dict):
                    source = list(source.values())[0]
                targets = ast.literal_eval(row[destination_column])
                if isinstance(targets, dict):
                    for target in targets.values():
                        query = self._generate_query(
                            source, target, relationship_type, properties,
                            source_label, destination_label, standard_id,
                            source_column, destination_column
                        )
                        yield query
            elif rel_type == &#39;ENCODES&#39;:
                source = int(row[source_column])
                target = str(row[destination_column])
                query = self._generate_query(
                    source, target, relationship_type, properties, source_label,
                    destination_label, standard_id, source_column, destination_column
                )
                yield query
            else:
                source = int(row[source_column])
                target = int(row[destination_column])
                query = self._generate_query(
                    source, target, relationship_type, properties, source_label,
                    destination_label, standard_id, source_column, destination_column
                )
                yield query

        self.logger.info(&#34;Cypher queries generated successfully.&#34;)

    def generate_cypher_queries_from_directories(self, directory, rel_type,
                                                 source_label, destination_label,
                                                 rel_type_column=None):
        &#34;&#34;&#34;
        Generate Cypher queries for creating relationships in Neo4j by merging CSV files from a directory.

        Parameters
        ----------
        directory : str
            The path to the directory containing the CSV files.
        rel_type : str
            The type of the relationship.
        source_label : str
            The label for the source node.
        destination_label : str
            The label for the destination node.
        rel_type_column : str, optional
            The column name for the relationship type, if it is to be extracted from the CSV file.

        Yields
        ------
        str
            A Cypher query string.
        &#34;&#34;&#34;
        combined_df = self.combine_csv_files(directory)
        if combined_df.empty:
            self.logger.error(&#34;The directory %s is empty or contains no valid CSV files.&#34;,
                              directory)
            return []
        file_path = os.path.join(directory, directory.split(&#39;/&#39;)[-1] + &#34;.csv&#34;)
        combined_df.to_csv(file_path, index=False)
        return self.generate_cypher_queries_from_file(file_path,
                                                      rel_type, source_label,
                                                      destination_label,
                                                      rel_type_column)

    def execute_queries(self, queries, batch_size=100):
        &#34;&#34;&#34;
        Execute the provided list of Cypher queries against the Neo4j database.

        Parameters
        ----------
        queries : list
            A list of Cypher query strings to execute.
        batch_size : int, optional
            The number of queries to execute in each batch.
        &#34;&#34;&#34;
        if not queries:
            self.logger.error(&#34;No queries to execute.&#34;)
            return

        self.logger.info(&#34;Executing Cypher queries...&#34;)
        with self.driver.session() as session:
            for i in range(0, len(queries), batch_size):
                batch = queries[i:i + batch_size]
                try:
                    for query in batch:
                        session.run(query)
                        self.logger.debug(f&#34;Executed query: {query}&#34;)
                    self.logger.info(f&#34;Executed batch {i // batch_size + 1}&#34;
                                     f&#34;of {len(queries) // batch_size + 1}&#34;)
                except Exception as e:
                    self.logger.error(&#34;Failed to execute batch starting at query %s: %s&#34;,
                                      i, str(e))

        self.logger.info(&#34;All queries executed.&#34;)

    def combine_csv_files(self, input_directory):
        &#34;&#34;&#34;
        Combine multiple CSV files with the same columns into a single DataFrame.

        Parameters
        ----------
        input_directory : str
            The directory containing the CSV files to be combined.

        Returns
        -------
        pandas.DataFrame
            A combined DataFrame containing data from all the CSV files.
        &#34;&#34;&#34;
        self.logger.info(&#34;Combining CSV files from directory: %s&#34;, input_directory)
        dfs = [
            pd.read_csv(os.path.join(input_directory, file))
            for file in os.listdir(input_directory)
            if file.endswith(&#34;.csv&#34;)
        ]
        if not dfs:
            self.logger.error(&#34;No valid CSV files found in the directory %s.&#34;,
                              input_directory)
            return pd.DataFrame()

        combined_df = pd.concat(dfs, ignore_index=True)
        self.logger.info(&#34;Successfully combined CSV files.&#34;)
        return combined_df

    def process_and_add_relationships(self, file_path, rel_type, source_label,
                                      destination_label, rel_type_column=None):
        &#34;&#34;&#34;
        Process the CSV file and add relationship data to the Neo4j database.

        Parameters
        ----------
        file_path : str
            The path to the CSV file.
        rel_type : str
            The type of the relationship.
        source_label : str
            The label for the source node.
        destination_label : str
            The label for the destination node.
        rel_type_column : str, optional
            The column name for the relationship type, if it is to be extracted from the CSV file.
        &#34;&#34;&#34;
        self.logger.info(&#34;Processing and adding relationships from file: %s&#34;,
                         file_path)
        queries = list(self.generate_cypher_queries_from_file(file_path,
                                                              rel_type,
                                                              source_label,
                                                              destination_label,
                                                              rel_type_column))
        self.execute_queries(queries)
        self.logger.info(&#34;Successfully processed and added relationships from file: %s&#34;,
                         file_path)

    def process_and_add_relationships_from_directory(self, directory_path,
                                                     rel_type, source_label,
                                                     destination_label,
                                                     rel_type_column=None):
        &#34;&#34;&#34;
        Combine CSV files from a directory and add relationship data to the Neo4j database.

        Parameters
        ----------
        directory_path : str
            The path to the directory containing the CSV files.
        rel_type : str
            The type of the relationship.
        source_label : str
            The label for the source node.
        destination_label : str
            The label for the destination node.
        rel_type_column : str, optional
            The column name for the relationship type, if it is to be extracted from the CSV file.
        &#34;&#34;&#34;
        self.logger.info(&#34;Processing and adding relationships from directory: %s&#34;,
                         directory_path)
        queries = list(self.generate_cypher_queries_from_directories(directory_path,
                                                                     rel_type,
                                                                     source_label,
                                                                     destination_label,
                                                                     rel_type_column))
        if not queries:
            self.logger.error(&#34;No valid relationships found in the directory %s.&#34;,
                              directory_path)
            return

        self.execute_queries(queries)
        self.logger.info(&#34;Successfully processed and added relationships from directory: %s&#34;,
                         directory_path)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="chemgraphbuilder.neo4jdriver.Neo4jBase" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase">Neo4jBase</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.AddGraphRelationships.combine_csv_files"><code class="name flex">
<span>def <span class="ident">combine_csv_files</span></span>(<span>self, input_directory)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine multiple CSV files with the same columns into a single DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_directory</code></strong> :&ensp;<code>str</code></dt>
<dd>The directory containing the CSV files to be combined.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A combined DataFrame containing data from all the CSV files.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships.execute_queries"><code class="name flex">
<span>def <span class="ident">execute_queries</span></span>(<span>self, queries, batch_size=100)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute the provided list of Cypher queries against the Neo4j database.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>queries</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of Cypher query strings to execute.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of queries to execute in each batch.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_directories"><code class="name flex">
<span>def <span class="ident">generate_cypher_queries_from_directories</span></span>(<span>self, directory, rel_type, source_label, destination_label, rel_type_column=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Cypher queries for creating relationships in Neo4j by merging CSV files from a directory.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>directory</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the directory containing the CSV files.</dd>
<dt><strong><code>rel_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the relationship.</dd>
<dt><strong><code>source_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the source node.</dd>
<dt><strong><code>destination_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the destination node.</dd>
<dt><strong><code>rel_type_column</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column name for the relationship type, if it is to be extracted from the CSV file.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>str</code></dt>
<dd>A Cypher query string.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_file"><code class="name flex">
<span>def <span class="ident">generate_cypher_queries_from_file</span></span>(<span>self, file_path, rel_type, source_label, destination_label, rel_type_column=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate Cypher queries for creating relationships in Neo4j based on the provided CSV file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the CSV file.</dd>
<dt><strong><code>rel_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the relationship.</dd>
<dt><strong><code>source_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the source node.</dd>
<dt><strong><code>destination_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the destination node.</dd>
<dt><strong><code>rel_type_column</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column name for the relationship type, if it is to be extracted from the CSV file.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>str</code></dt>
<dd>A Cypher query string.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships.process_and_add_relationships"><code class="name flex">
<span>def <span class="ident">process_and_add_relationships</span></span>(<span>self, file_path, rel_type, source_label, destination_label, rel_type_column=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Process the CSV file and add relationship data to the Neo4j database.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the CSV file.</dd>
<dt><strong><code>rel_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the relationship.</dd>
<dt><strong><code>source_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the source node.</dd>
<dt><strong><code>destination_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the destination node.</dd>
<dt><strong><code>rel_type_column</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column name for the relationship type, if it is to be extracted from the CSV file.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.AddGraphRelationships.process_and_add_relationships_from_directory"><code class="name flex">
<span>def <span class="ident">process_and_add_relationships_from_directory</span></span>(<span>self, directory_path, rel_type, source_label, destination_label, rel_type_column=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine CSV files from a directory and add relationship data to the Neo4j database.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>directory_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the directory containing the CSV files.</dd>
<dt><strong><code>rel_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the relationship.</dd>
<dt><strong><code>source_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the source node.</dd>
<dt><strong><code>destination_label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label for the destination node.</dd>
<dt><strong><code>rel_type_column</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column name for the relationship type, if it is to be extracted from the CSV file.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="chemgraphbuilder.neo4jdriver.Neo4jBase" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase">Neo4jBase</a></b></code>:
<ul class="hlist">
<li><code><a title="chemgraphbuilder.neo4jdriver.Neo4jBase.close" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase.close">close</a></code></li>
<li><code><a title="chemgraphbuilder.neo4jdriver.Neo4jBase.connect_to_neo4j" href="neo4jdriver.html#chemgraphbuilder.neo4jdriver.Neo4jBase.connect_to_neo4j">connect_to_neo4j</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="chemgraphbuilder.GraphNodesLoader"><code class="flex name class">
<span>class <span class="ident">GraphNodesLoader</span></span>
<span>(</span><span>uri, username, password)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to load data into a Neo4j graph database for different node types.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>driver</code></strong></dt>
<dd>The Neo4j driver instance.</dd>
<dt><strong><code>node_data_adder</code></strong></dt>
<dd>An instance of the AddGraphNodes class.</dd>
<dt><strong><code>label_mapping</code></strong></dt>
<dd>A dictionary mapping node labels to their unique properties and file paths.</dd>
</dl>
<p>Initializes the GraphDataLoader with Neo4j connection details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uri</code></strong> :&ensp;<code>str</code></dt>
<dd>The URI of the Neo4j database.</dd>
<dt><strong><code>username</code></strong> :&ensp;<code>str</code></dt>
<dd>The username for the Neo4j database.</dd>
<dt><strong><code>password</code></strong> :&ensp;<code>str</code></dt>
<dd>The password for the Neo4j database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GraphNodesLoader:
    &#34;&#34;&#34;
    Class to load data into a Neo4j graph database for different node types.

    Attributes:
        driver: The Neo4j driver instance.
        node_data_adder: An instance of the AddGraphNodes class.
        label_mapping: A dictionary mapping node labels to their unique properties and file paths.
    &#34;&#34;&#34;

    def __init__(self, uri, username, password):
        &#34;&#34;&#34;
        Initializes the GraphDataLoader with Neo4j connection details.

        Args:
            uri (str): The URI of the Neo4j database.
            username (str): The username for the Neo4j database.
            password (str): The password for the Neo4j database.
        &#34;&#34;&#34;
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.node_data_adder = AddGraphNodes(self.driver)
        self.label_mapping = {
            &#34;Compound&#34;: {
                &#34;unique_property&#34;: &#34;CompoundID&#34;,
                &#34;file_path&#34;: &#34;Data/Nodes/Compound_Properties_Processed.csv&#34;
            },
            &#34;BioAssay&#34;: {
                &#34;unique_property&#34;: &#34;AssayID&#34;,
                &#34;file_path&#34;: &#34;Data/Nodes/Assay_Properties_Processed.csv&#34;
            },
            &#34;Gene&#34;: {
                &#34;unique_property&#34;: &#34;GeneID&#34;,
                &#34;file_path&#34;: &#34;Data/Nodes/Gene_Properties_Processed.csv&#34;
            },
            &#34;Protein&#34;: {
                &#34;unique_property&#34;: &#34;ProteinRefSeqAccession&#34;,
                &#34;file_path&#34;: &#34;Data/Nodes/Protein_Properties_Processed.csv&#34;
            }
        }

    def create_uniqueness_constraint(self, label, unique_property):
        &#34;&#34;&#34;
        Creates a uniqueness constraint for a given node label and property.

        Args:
            label (str): The label of the node.
            unique_property (str): The property to enforce uniqueness on.
        &#34;&#34;&#34;
        self.node_data_adder.create_uniqueness_constraint(
            self.driver, label=label, unique_property=unique_property
        )

    def process_and_add_nodes(self, file_path, label, unique_property):
        &#34;&#34;&#34;
        Processes and adds nodes from a CSV file to the Neo4j database.

        Args:
            file_path (str): The path to the CSV file containing node data.
            label (str): The label of the node.
            unique_property (str): The unique property of the node.
        &#34;&#34;&#34;
        self.node_data_adder.process_and_add_nodes(
            file_path, label=label, unique_property=unique_property
        )

    def load_data_for_node_type(self, label):
        &#34;&#34;&#34;
        Loads data for a specific node type into the Neo4j database.

        Args:
            label (str): The label of the node.
        &#34;&#34;&#34;
        if label not in self.label_mapping:
            self.logger.error(f&#34;No mapping found for label: {label}&#34;)
            return

        unique_property = self.label_mapping[label][&#34;unique_property&#34;]
        file_path = self.label_mapping[label][&#34;file_path&#34;]

        self.create_uniqueness_constraint(label, unique_property)
        self.process_and_add_nodes(file_path, label, unique_property)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.GraphNodesLoader.create_uniqueness_constraint"><code class="name flex">
<span>def <span class="ident">create_uniqueness_constraint</span></span>(<span>self, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a uniqueness constraint for a given node label and property.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label of the node.</dd>
<dt><strong><code>unique_property</code></strong> :&ensp;<code>str</code></dt>
<dd>The property to enforce uniqueness on.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.GraphNodesLoader.load_data_for_node_type"><code class="name flex">
<span>def <span class="ident">load_data_for_node_type</span></span>(<span>self, label)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads data for a specific node type into the Neo4j database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label of the node.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.GraphNodesLoader.process_and_add_nodes"><code class="name flex">
<span>def <span class="ident">process_and_add_nodes</span></span>(<span>self, file_path, label, unique_property)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes and adds nodes from a CSV file to the Neo4j database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the CSV file containing node data.</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>The label of the node.</dd>
<dt><strong><code>unique_property</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique property of the node.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.GraphRelationshipsLoader"><code class="flex name class">
<span>class <span class="ident">GraphRelationshipsLoader</span></span>
<span>(</span><span>uri, username, password)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for loading graph relationships into a Neo4j database using the AddGraphRelationships class.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>uri</code></strong> :&ensp;<code>str</code></dt>
<dd>The URI for the Neo4j database.</dd>
<dt><strong><code>username</code></strong> :&ensp;<code>str</code></dt>
<dd>The username for the Neo4j database.</dd>
<dt><strong><code>password</code></strong> :&ensp;<code>str</code></dt>
<dd>The password for the Neo4j database.</dd>
<dt><strong><code>relationship_settings</code></strong> :&ensp;<code>dict</code></dt>
<dd>Predefined settings for different relationship types.</dd>
</dl>
<p>Initializes the GraphRelationshipsLoader with Neo4j connection details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uri</code></strong> :&ensp;<code>str</code></dt>
<dd>The URI for the Neo4j database.</dd>
<dt><strong><code>username</code></strong> :&ensp;<code>str</code></dt>
<dd>The username for the Neo4j database.</dd>
<dt><strong><code>password</code></strong> :&ensp;<code>str</code></dt>
<dd>The password for the Neo4j database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GraphRelationshipsLoader:
    &#34;&#34;&#34;
    Class for loading graph relationships into a Neo4j database using the AddGraphRelationships class.

    Attributes:
        uri (str): The URI for the Neo4j database.
        username (str): The username for the Neo4j database.
        password (str): The password for the Neo4j database.
        relationship_settings (dict): Predefined settings for different relationship types.
    &#34;&#34;&#34;

    def __init__(self, uri, username, password):
        &#34;&#34;&#34;
        Initializes the GraphRelationshipsLoader with Neo4j connection details.

        Args:
            uri (str): The URI for the Neo4j database.
            username (str): The username for the Neo4j database.
            password (str): The password for the Neo4j database.
        &#34;&#34;&#34;
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        handler.setLevel(logging.INFO)
        formatter = logging.Formatter(&#39;%(asctime)s - %(levelname)s - %(message)s&#39;)
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.add_graph_relationships = AddGraphRelationships(self.driver)
        self.logger.info(&#34;GraphRelationshipsLoader class initialized.&#34;)

        # Predefined settings for different relationship types
        self.relationship_settings = {
            &#34;Compound_Gene&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Compound_Gene_Relationship.csv&#34;,
                &#34;source_label&#34;: &#34;Compound&#34;,
                &#34;destination_label&#34;: &#34;Gene&#34;,
                &#34;rel_type_column&#34;: &#34;Activity&#34;
            },
            &#34;Assay_Compound&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Assay_Compound_Relationship.csv&#34;,
                &#34;source_label&#34;: &#34;BioAssay&#34;,
                &#34;destination_label&#34;: &#34;Compound&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;EVALUATES&#34;
            },
            &#34;Assay_Gene&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Assay_Gene_Relationship.csv&#34;,
                &#34;source_label&#34;: &#34;BioAssay&#34;,
                &#34;destination_label&#34;: &#34;Gene&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;STUDIES&#34;
            },
            &#34;Compound_Transformation&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Compound_Transformation.csv&#34;,
                &#34;source_label&#34;: &#34;Compound&#34;,
                &#34;destination_label&#34;: &#34;Compound&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;IS_METABOLIZED_TO&#34;
            },
            &#34;Gene_Enzyme&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Gene_Enzyme_Relationship.csv&#34;,
                &#34;source_label&#34;: &#34;Gene&#34;,
                &#34;destination_label&#34;: &#34;Protein&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;ENCODES&#34;
            },
            &#34;Compound_Similarities&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Compound_Similarities&#34;,
                &#34;source_label&#34;: &#34;Compound&#34;,
                &#34;destination_label&#34;: &#34;Compound&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;IS_SIMILAR_TO&#34;,
                &#34;is_directory&#34;: True
            },
            &#34;Cpd_Cpd_CoOccurence&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Cpd_Cpd_CoOccurence&#34;,
                &#34;source_label&#34;: &#34;Compound&#34;,
                &#34;destination_label&#34;: &#34;Compound&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;CO_OCCURS_IN_LITERATURE&#34;,
                &#34;is_directory&#34;: True
            },
            &#34;Cpd_Gene_CoOccurence&#34;: {
                &#34;file_path&#34;: &#34;Data/Relationships/Cpd_Gene_CoOccurence&#34;,
                &#34;source_label&#34;: &#34;Compound&#34;,
                &#34;destination_label&#34;: &#34;Gene&#34;,
                &#34;rel_type_column&#34;: None,
                &#34;relationship_type&#34;: &#34;CO_OCCURS_IN_LITERATURE&#34;,
                &#34;is_directory&#34;: True
            }
        }

    def close(self):
        &#34;&#34;&#34;Closes the Neo4j database driver connection.&#34;&#34;&#34;
        self.driver.close()

    def add_relationships(self, relationship_type):
        &#34;&#34;&#34;
        Adds relationships to the Neo4j database based on the specified relationship type.

        Args:
            relationship_type (str): The type of the relationship.
        &#34;&#34;&#34;
        settings = self.relationship_settings.get(relationship_type)
        if not settings:
            self.logger.error(f&#34;Invalid relationship type: {relationship_type}&#34;)
            return

        file_path = settings[&#34;file_path&#34;]
        source_label = settings[&#34;source_label&#34;]
        destination_label = settings[&#34;destination_label&#34;]
        rel_type_column = settings.get(&#34;rel_type_column&#34;)
        is_directory = settings.get(&#34;is_directory&#34;, False)

        if is_directory:
            self.add_graph_relationships.process_and_add_relationships_from_directory(file_path,
                                                                                      settings[&#34;relationship_type&#34;],
                                                                                      source_label,
                                                                                      destination_label,
                                                                                      rel_type_column)
        else:
            self.add_graph_relationships.process_and_add_relationships(file_path,
                                                                       settings.get(&#34;relationship_type&#34;),
                                                                       source_label,
                                                                       destination_label,
                                                                       rel_type_column)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.GraphRelationshipsLoader.add_relationships"><code class="name flex">
<span>def <span class="ident">add_relationships</span></span>(<span>self, relationship_type)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds relationships to the Neo4j database based on the specified relationship type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>relationship_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the relationship.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.GraphRelationshipsLoader.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Closes the Neo4j database driver connection.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.Neo4jBase"><code class="flex name class">
<span>class <span class="ident">Neo4jBase</span></span>
<span>(</span><span>logger=None, uri='tcp://5.tcp.eu.ngrok.io:12445', user='neo4j')</span>
</code></dt>
<dd>
<div class="desc"><p>Base class to manage connections with the Neo4j database.</p>
<p>Attributes:
- uri: The connection URI for the Neo4j database.
- user: The username to use for authentication.
- driver: The driver object used to interact with the Neo4j database.</p>
<p>Methods:
- connect_to_neo4j: Establish a connection to the Neo4j database.
- close: Close the connection to the Neo4j database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Neo4jBase:
    &#34;&#34;&#34;
    Base class to manage connections with the Neo4j database.

    Attributes:
    - uri: The connection URI for the Neo4j database.
    - user: The username to use for authentication.
    - driver: The driver object used to interact with the Neo4j database.

    Methods:
    - connect_to_neo4j: Establish a connection to the Neo4j database.
    - close: Close the connection to the Neo4j database.
    &#34;&#34;&#34;

    def __init__(self, logger=None, uri=&#34;tcp://5.tcp.eu.ngrok.io:12445&#34;, user=&#34;neo4j&#34;):
        self.uri = uri
        self.user = user
        self.driver = None

        # Set up logging configuration
        logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;)
        self.logger = logger or logging.getLogger(__name__)

    def connect_to_neo4j(self):
        &#34;&#34;&#34;Establish a connection to the Neo4j database using provided URI and username.&#34;&#34;&#34;
        password = os.getenv(&#34;NEO4J_PASSWORD&#34;)  # Check if password is set in environment variables
        if not password:
            password = getpass.getpass(prompt=&#34;Enter Neo4j password: &#34;)

        try:
            self.driver = GraphDatabase.driver(self.uri, auth=(self.user, password))
            self.logger.info(&#34;Successfully connected to the Neo4j database.&#34;)
        except Exception as e:
            self.logger.error(&#34;Failed to connect to the Neo4j database: %s&#34;, e)
            raise Neo4jConnectionError(&#34;Failed to connect to the Neo4j database.&#34;) from e

    def close(self):
        &#34;&#34;&#34;Close the connection to the Neo4j database.&#34;&#34;&#34;
        if self.driver:
            self.driver.close()
            self.logger.info(&#34;Neo4j connection closed successfully.&#34;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="chemgraphbuilder.add_graph_nodes.AddGraphNodes" href="add_graph_nodes.html#chemgraphbuilder.add_graph_nodes.AddGraphNodes">AddGraphNodes</a></li>
<li><a title="chemgraphbuilder.add_graph_relationships.AddGraphRelationships" href="add_graph_relationships.html#chemgraphbuilder.add_graph_relationships.AddGraphRelationships">AddGraphRelationships</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.Neo4jBase.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Close the connection to the Neo4j database.</p></div>
</dd>
<dt id="chemgraphbuilder.Neo4jBase.connect_to_neo4j"><code class="name flex">
<span>def <span class="ident">connect_to_neo4j</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Establish a connection to the Neo4j database using provided URI and username.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.NodeCollectorProcessor"><code class="flex name class">
<span>class <span class="ident">NodeCollectorProcessor</span></span>
<span>(</span><span>uri, username, password, node_type, enzyme_list)</span>
</code></dt>
<dd>
<div class="desc"><p>A class to collect and process data for different types of nodes using NodePropertiesExtractor and NodeDataProcessor.</p>
<p>Initializes the NodesCollectorProcessor with Neo4j connection details, the node type to collect data for, and the list of enzymes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>uri</code></strong> :&ensp;<code>str</code></dt>
<dd>The URI for the Neo4j database.</dd>
<dt><strong><code>username</code></strong> :&ensp;<code>str</code></dt>
<dd>The username for the Neo4j database.</dd>
<dt><strong><code>password</code></strong> :&ensp;<code>str</code></dt>
<dd>The password for the Neo4j database.</dd>
<dt><strong><code>node_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of node to collect data for (e.g., 'Compound', 'BioAssay', 'Gene', 'Protein').</dd>
<dt><strong><code>enzyme_list</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>List of enzyme names for which assay data will be fetched from PubChem.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NodeCollectorProcessor:
    &#34;&#34;&#34;
    A class to collect and process data for different types of nodes using NodePropertiesExtractor and NodeDataProcessor.
    &#34;&#34;&#34;

    def __init__(self, uri, username, password, node_type, enzyme_list):
        &#34;&#34;&#34;
        Initializes the NodesCollectorProcessor with Neo4j connection details, the node type to collect data for, and the list of enzymes.

        Args:
            uri (str): The URI for the Neo4j database.
            username (str): The username for the Neo4j database.
            password (str): The password for the Neo4j database.
            node_type (str): The type of node to collect data for (e.g., &#39;Compound&#39;, &#39;BioAssay&#39;, &#39;Gene&#39;, &#39;Protein&#39;).
            enzyme_list (list of str): List of enzyme names for which assay data will be fetched from PubChem.
        &#34;&#34;&#34;
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.node_type = node_type
        self.extractor = NodePropertiesExtractor(enzyme_list=enzyme_list)
        self.processor = NodeDataProcessor(data_dir=&#34;Data&#34;)

    def close(self):
        &#34;&#34;&#34;Closes the connection to the Neo4j database.&#34;&#34;&#34;
        self.driver.close()

    def collect_and_process_data(self):
        &#34;&#34;&#34;
        Collects and processes data based on the node type and saves it to the appropriate file.
        &#34;&#34;&#34;
        if self.node_type == &#39;Compound&#39;:
            self.extractor.create_data_directories()
            df = self.extractor.run()
            self.extractor.extract_compound_properties(main_data=&#39;Data/AllDataConnected.csv&#39;)
            self.processor.preprocess_compounds()
        elif self.node_type == &#39;BioAssay&#39;:
            self.extractor.create_data_directories()
            df = self.extractor.run()
            self.extractor.extract_assay_properties(main_data=&#39;Data/AllDataConnected.csv&#39;)
            self.processor.preprocess_assays()
        elif self.node_type == &#39;Gene&#39;:
            self.extractor.create_data_directories()
            df = self.extractor.run()
            self.extractor.extract_gene_properties(main_data=&#39;Data/AllDataConnected.csv&#39;)
            self.processor.preprocess_genes()
        elif self.node_type == &#39;Protein&#39;:
            self.extractor.create_data_directories()
            df = self.extractor.run()
            self.extractor.extract_protein_properties(main_data=&#39;Data/AllDataConnected.csv&#39;)
            self.processor.preprocess_proteins()
        else:
            logging.error(f&#34;Unsupported node type: {self.node_type}&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.NodeCollectorProcessor.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Closes the connection to the Neo4j database.</p></div>
</dd>
<dt id="chemgraphbuilder.NodeCollectorProcessor.collect_and_process_data"><code class="name flex">
<span>def <span class="ident">collect_and_process_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Collects and processes data based on the node type and saves it to the appropriate file.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.NodeDataProcessor"><code class="flex name class">
<span>class <span class="ident">NodeDataProcessor</span></span>
<span>(</span><span>data_dir: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the NodeDataPreprocessor with a directory path to manage the data files.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>The directory where the node data files are stored.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NodeDataProcessor:
    def __init__(self, data_dir: str):
        &#34;&#34;&#34;
        Initializes the NodeDataPreprocessor with a directory path to manage the data files.

        Args:
            data_dir (str): The directory where the node data files are stored.
        &#34;&#34;&#34;
        self.data_dir = data_dir

    def preprocess_assays(self):
        &#34;&#34;&#34;
        Processes the assay data by renaming columns and saving the modified data back to disk.
        This method also handles visualization of assay data distributions if necessary.
        &#34;&#34;&#34;
        df = pd.read_csv(f&#39;{self.data_dir}/Nodes/Assay_Properties.csv&#39;)
        df.rename(columns={&#34;AID&#34;: &#34;AssayID&#34;, &#34;Assay Type&#34;: &#34;AssayType&#34;,
                           &#34;Activity Name&#34;: &#34;AssayActivityName&#34;, &#34;SourceID&#34;: &#34;AssaySourceID&#34;,
                           &#34;SourceName&#34;: &#34;AssaySourceName&#34;, &#34;Name&#34;: &#34;AssayName&#34;,
                           &#34;Description&#34;: &#34;AssayDescription&#34;}, inplace=True)
        df.to_csv(f&#39;{self.data_dir}/Nodes/Assay_Properties_Processed.csv&#39;, index=False)

    def preprocess_proteins(self):
        &#34;&#34;&#34;
        Processes the protein data by renaming columns and saving the processed data.
        This method simplifies access to protein data for downstream analysis.
        &#34;&#34;&#34;
        df = pd.read_csv(f&#39;{self.data_dir}/Nodes/Protein_Properties.csv&#39;)
        df.rename(columns={&#34;ID&#34;: &#34;ProteinID&#34;, &#34;Name&#34;: &#34;ProteinName&#34;,
                           &#34;Description&#34;: &#34;ProteinDescription&#34;}, inplace=True)
        df.to_csv(f&#39;{self.data_dir}/Nodes/Protein_Properties_Processed.csv&#39;, index=False)

    def preprocess_genes(self):
        &#34;&#34;&#34;
        Processes gene data by renaming columns and changing data types for specific fields.
        The processed data is saved for further use in gene-related analyses.
        &#34;&#34;&#34;
        df = pd.read_csv(f&#39;{self.data_dir}/Nodes/Gene_Properties.csv&#39;)
        df.rename(columns={&#34;Symbol&#34;: &#34;GeneSymbol&#34;, &#34;Taxonomy ID&#34;: &#34;TaxonomyID&#34;,
                           &#34;Synonyms&#34;: &#34;GeneSynonyms&#34;}, inplace=True)
        df[&#39;GeneID&#39;] = df[&#39;GeneID&#39;].astype(&#39;Int64&#39;)
        df.to_csv(f&#39;{self.data_dir}/Nodes/Gene_Properties_Processed.csv&#39;, index=False)

    def preprocess_compounds(self):
        &#34;&#34;&#34;
        Concatenates multiple CSV files containing compound data into a single file,
        renames columns for uniformity, and saves the consolidated data. This method
        facilitates easier management and analysis of compound data.
        &#34;&#34;&#34;
        path = f&#39;{self.data_dir}/Nodes/Compound_Properties&#39;
        all_csv_files = glob.glob(path + &#34;/*.csv&#34;)
        first_file = True
        output_file = f&#39;{path}/Compound_Properties.csv&#39;

        with open(output_file, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;utf-8&#39;) as f_out:
            for file in all_csv_files:
                with open(file, &#39;r&#39;, newline=&#39;&#39;, encoding=&#39;utf-8&#39;) as f_in:
                    header = f_in.readline()
                    if first_file:
                        f_out.write(header)
                        first_file = False
                    for line in f_in:
                        f_out.write(line)

        df = pd.read_csv(output_file)
        df.rename(columns={&#34;CID&#34;: &#34;CompoundID&#34;, &#34;Title&#34;: &#34;CompoundName&#34;}, inplace=True)
        df.to_csv(f&#39;{path}/Compound_Properties_Processed.csv&#39;, index=False)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.NodeDataProcessor.preprocess_assays"><code class="name flex">
<span>def <span class="ident">preprocess_assays</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes the assay data by renaming columns and saving the modified data back to disk.
This method also handles visualization of assay data distributions if necessary.</p></div>
</dd>
<dt id="chemgraphbuilder.NodeDataProcessor.preprocess_compounds"><code class="name flex">
<span>def <span class="ident">preprocess_compounds</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates multiple CSV files containing compound data into a single file,
renames columns for uniformity, and saves the consolidated data. This method
facilitates easier management and analysis of compound data.</p></div>
</dd>
<dt id="chemgraphbuilder.NodeDataProcessor.preprocess_genes"><code class="name flex">
<span>def <span class="ident">preprocess_genes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes gene data by renaming columns and changing data types for specific fields.
The processed data is saved for further use in gene-related analyses.</p></div>
</dd>
<dt id="chemgraphbuilder.NodeDataProcessor.preprocess_proteins"><code class="name flex">
<span>def <span class="ident">preprocess_proteins</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes the protein data by renaming columns and saving the processed data.
This method simplifies access to protein data for downstream analysis.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor"><code class="flex name class">
<span>class <span class="ident">NodePropertiesExtractor</span></span>
<span>(</span><span>enzyme_list, base_url='https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol', sep=',')</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts data from PubChem to build knowledge graphs in Neo4j, focusing on nodes representing chemical
entities and their relationships. This class serves as a bridge between the PubChem database and Neo4j,
allowing users to query chemical data and construct a graph-based representation of chemical compounds,
their assays, related genes, and proteins.</p>
<p>The primary functionality revolves around fetching detailed information about specified enzymes from PubChem,
including assay data, gene properties, protein properties, and compound properties. It processes this data into
a structured format suitable for knowledge graph construction, specifically tailored for use with Neo4j databases.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>enzyme_list</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Enzymes to query in the PubChem database.</dd>
<dt><strong><code>_base_url</code></strong> :&ensp;<code>str</code></dt>
<dd>Base URL for the PubChem API requests.</dd>
<dt><strong><code>_sep</code></strong> :&ensp;<code>str</code></dt>
<dd>Delimiter for parsing CSV data from PubChem.</dd>
<dt><strong><code>_enzyme_count</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of enzymes in the enzyme_list, calculated at initialization.</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<p>enzyme_list (list of str): List of enzyme names for which assay data will be fetched from PubChem.
base_url (str, optional): Base URL for PubChem API requests. Defaults to the assay target genesymbol endpoint.
sep (str, optional): Separator used for parsing CSV data returned by PubChem. Defaults to ','.</p>
<p>Usage Example:
&gt;&gt;&gt; enzyme_list = ['CYP2D6', 'CYP3A4']
&gt;&gt;&gt; extractor = NodePropertiesExtractor(enzyme_list)
&gt;&gt;&gt; df = extractor.run()
This example initiates the extractor with a list of enzymes, fetches their data from PubChem,
processes it, and potentially prepares it for knowledge graph construction in Neo4j.</p>
<h2 id="note">Note</h2>
<p>To fully utilize this class, ensure you have network access to the PubChem API for data retrieval and
a Neo4j database instance for knowledge graph construction. The class methods facilitate data extraction
and processing, but integrating the output into Neo4j requires additional steps outside the scope of this class.</p>
<p>Initializes a NodePropertiesExtractor instance, setting up the base URL for API requests, the separator for CSV parsing,
and the list of enzymes to query from the PubChem database.</p>
<h2 id="parameters_1">Parameters</h2>
<p>enzyme_list (list of str): A list of enzyme names for which to fetch assay data.
base_url (str, optional): The base URL for PubChem API requests.
Default is set to the assay target genesymbol endpoint.
sep (str, optional): The delimiter to use for parsing CSV files returned by PubChem. Defaults to ','.</p>
<h2 id="attributes_1">Attributes</h2>
<dl>
<dt><strong><code>_base_url</code></strong> :&ensp;<code>str</code></dt>
<dd>Stores the base URL for API requests.</dd>
<dt><strong><code>_sep</code></strong> :&ensp;<code>str</code></dt>
<dd>Stores the delimiter for parsing CSV data.</dd>
<dt><strong><code>enzyme_list</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Stores the list of enzyme names provided during initialization.</dd>
<dt><strong><code>_enzyme_count</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of enzymes in the enzyme_list.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NodePropertiesExtractor:
    &#34;&#34;&#34;
    Extracts data from PubChem to build knowledge graphs in Neo4j, focusing on nodes representing chemical
    entities and their relationships. This class serves as a bridge between the PubChem database and Neo4j,
    allowing users to query chemical data and construct a graph-based representation of chemical compounds,
    their assays, related genes, and proteins.

    The primary functionality revolves around fetching detailed information about specified enzymes from PubChem,
    including assay data, gene properties, protein properties, and compound properties. It processes this data into
    a structured format suitable for knowledge graph construction, specifically tailored for use with Neo4j databases.

    Attributes:
        enzyme_list (list of str): Enzymes to query in the PubChem database.
        _base_url (str): Base URL for the PubChem API requests.
        _sep (str): Delimiter for parsing CSV data from PubChem.
        _enzyme_count (int): Number of enzymes in the enzyme_list, calculated at initialization.

    Parameters:
        enzyme_list (list of str): List of enzyme names for which assay data will be fetched from PubChem.
        base_url (str, optional): Base URL for PubChem API requests. Defaults to the assay target genesymbol endpoint.
        sep (str, optional): Separator used for parsing CSV data returned by PubChem. Defaults to &#39;,&#39;.

    Usage Example:
        &gt;&gt;&gt; enzyme_list = [&#39;CYP2D6&#39;, &#39;CYP3A4&#39;]
        &gt;&gt;&gt; extractor = NodePropertiesExtractor(enzyme_list)
        &gt;&gt;&gt; df = extractor.run()
        This example initiates the extractor with a list of enzymes, fetches their data from PubChem,
        processes it, and potentially prepares it for knowledge graph construction in Neo4j.

    Note:
        To fully utilize this class, ensure you have network access to the PubChem API for data retrieval and
        a Neo4j database instance for knowledge graph construction. The class methods facilitate data extraction
        and processing, but integrating the output into Neo4j requires additional steps outside the scope of this class.
    &#34;&#34;&#34;

    _REQUEST_TIMEOUT = 30  # in seconds
    _CONCURRENT_REQUEST_LIMIT = 2
    _RETRY_ATTEMPTS = 3  # number of times to retry a failed request

    def __init__(self, enzyme_list, base_url=&#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol&#34;, sep=&#34;,&#34;):
        &#34;&#34;&#34;
        Initializes a NodePropertiesExtractor instance, setting up the base URL for API requests, the separator for CSV parsing,
        and the list of enzymes to query from the PubChem database.

        Parameters:
            enzyme_list (list of str): A list of enzyme names for which to fetch assay data.
            base_url (str, optional): The base URL for PubChem API requests.
                                    Default is set to the assay target genesymbol endpoint.
            sep (str, optional): The delimiter to use for parsing CSV files returned by PubChem. Defaults to &#39;,&#39;.

        Attributes:
            _base_url (str): Stores the base URL for API requests.
            _sep (str): Stores the delimiter for parsing CSV data.
            enzyme_list (list of str): Stores the list of enzyme names provided during initialization.
            _enzyme_count (int): The number of enzymes in the enzyme_list.
        &#34;&#34;&#34;
        self._base_url = base_url
        self._sep = sep
        self.enzyme_list = enzyme_list
        self._enzyme_count = len(enzyme_list)

    def _make_request(self, url):
        &#34;&#34;&#34;
        Sends an HTTP GET request to a specified URL with built-in retry logic. If the request fails,
        it retries the request up to a predefined number of attempts with exponential backoff to handle potential
        temporary network or server issues.

        The method attempts to gracefully handle server-side errors (HTTP 4XX/5XX responses) by raising an exception
        if the response status code indicates an error. For client-side errors (e.g., connectivity issues), it logs a warning
        and retries the request.

        Parameters:
            url (str): The complete URL to which the HTTP GET request is sent.

        Returns:
            requests.Response: The response object from the server if the request is successfully completed.

        Raises:
            requests.RequestException: If the request fails to complete successfully after the maximum number of retry attempts.
        &#34;&#34;&#34;
        for attempt in range(self._RETRY_ATTEMPTS):
            try:
                response = requests.get(url, timeout=self._REQUEST_TIMEOUT)
                response.raise_for_status()  # Checks for HTTP errors
                return response
            except requests.RequestException as e:
                logging.warning(f&#34;Attempt {attempt + 1} of {self._RETRY_ATTEMPTS} failed for URL: {url}. Error: {e}&#34;)
                if attempt + 1 == self._RETRY_ATTEMPTS:
                    raise  # All attempts failed; re-raise the last exception
                time.sleep(2 ** attempt)  # Exponential backoff


    def get_enzyme_assays(self, enzyme):
        &#34;&#34;&#34;
        Fetches assay data for a specified enzyme from the PubChem database and returns it as a pandas DataFrame.

        This method constructs a URL to query the PubChem database for concise assay data related to the given enzyme.
        It processes the CSV response into a DataFrame, which includes various assay data points provided by PubChem.

        Parameters:
            enzyme (str): The name of the enzyme for which assay data is requested. This name is used in the API query.

        Returns:
            pd.DataFrame: A DataFrame containing the assay data fetched from PubChem for the specified enzyme. The DataFrame
                        includes columns based on the CSV response from PubChem, such as assay ID, results, and conditions.
                        Returns None if no data is available or if an error occurs during data fetching or processing.

        Raises:
            requests.RequestException: If an error occurs during the HTTP request to the PubChem API.
            pd.errors.EmptyDataError: If the response from PubChem contains no data.

        Example:
            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;enzyme&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; enzyme_assays_df = extractor.get_enzyme_assays(&#39;enzyme&#39;)
            &gt;&gt;&gt; print(enzyme_assays_df.head())
        &#34;&#34;&#34;
        assays_url = f&#34;{self._base_url}/{enzyme}/concise/CSV&#34;
        logging.info(f&#34;Fetching assays for enzyme: {enzyme}&#34;)

        response = self._make_request(assays_url)

        assays_csv_string = response.text
        assays_csv_string_io = StringIO(assays_csv_string)
        try:
            assays_df = pd.read_csv(assays_csv_string_io, sep=self._sep, low_memory=False)
            logging.info(f&#34;Assays DataFrame for enzyme {enzyme} has shape: {assays_df.shape}&#34;)
            return assays_df
        except pd.errors.EmptyDataError:
            logging.warning(f&#34;No data available for enzyme {enzyme}.&#34;)
            return None


    def _process_enzymes(self, enzyme_list):
        &#34;&#34;&#34;
        Iterates over a list of enzyme names, fetching assay data for each enzyme and aggregating the results into a list of DataFrames.

        This method calls `get_enzyme_assays` for each enzyme in the provided list, collecting the assay data (if available)
        into a list of pandas DataFrames. This list can then be used for further processing or analysis.

        Parameters:
            enzyme_list (list of str): A list containing the names of enzymes for which to fetch assay data.

        Returns:
            list of pd.DataFrame: A list containing a pandas DataFrame for each enzyme for which assay data was successfully
                                fetched and processed. Each DataFrame includes the assay data from PubChem for that enzyme.
                                If no data is available for an enzyme, it is omitted from the list.
        &#34;&#34;&#34;
        df_list = [self.get_enzyme_assays(enzyme) for enzyme in enzyme_list]
        return [df for df in df_list if df is not None]


    def _concatenate_data(self, df_list):
        &#34;&#34;&#34;
        Concatenates a list of pandas DataFrames into a single DataFrame. This method is useful for aggregating
        data fetched from multiple sources or APIs into a unified structure. If the list is empty, it returns None
        to indicate that no data was aggregated.

        Parameters:
            df_list (List[pd.DataFrame]): A list of pandas DataFrames to concatenate. These DataFrames should
                                        have the same structure (columns) to ensure proper concatenation.

        Returns:
            pd.DataFrame or None: A single concatenated DataFrame comprising all rows from the input DataFrames,
                                indexed continuously. Returns None if the input list is empty, indicating there
                                is no data to concatenate.
        &#34;&#34;&#34;
        if df_list:
            return pd.concat(df_list, ignore_index=True)
        return None


    def run(self):
        &#34;&#34;&#34;
        Orchestrates the process of fetching, filtering, and aggregating assay data from PubChem for a predefined list of enzymes.

        This method iteratively queries PubChem for assay data corresponding to each enzyme specified in the `enzyme_list` attribute during class initialization. It performs the following steps for each enzyme:
        1. Constructs a query URL and fetches assay data from PubChem.
        2. Filters the fetched data based on predefined criteria (e.g., containing specific substrings in the assay name).
        3. Aggregates the filtered data into a single pandas DataFrame.
        4. Identifies enzymes for which data could not be fetched or were excluded based on filtering criteria, logging their names.

        The final aggregated DataFrame, containing assay data for all successfully processed enzymes, is then saved to a CSV file. This method facilitates the extraction and preprocessing of chemical assay data for further analysis or integration into knowledge graphs.

        Note:
            - This method relies on the successful response from PubChem for each enzyme query.
            - Enzymes with no available data or failing to meet the filtering criteria are excluded from the final DataFrame.
            - The output CSV file is saved in the current working directory with the name &#39;Data/AllDataConnected.csv&#39;.

        Returns:
            pd.DataFrame: A DataFrame containing the aggregated and filtered assay data for the specified enzymes.
                        Columns in the DataFrame correspond to the assay data fields returned by PubChem, subject to
                        the filtering criteria applied within this method.

        Raises:
            requests.RequestException: If there is an issue with fetching data from PubChem, such as a network problem or
                                    an invalid response.

        Example:
            Assuming `enzyme_list` was set to [&#39;CYP2D6&#39;, &#39;CYP3A4&#39;] during class initialization:

            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; result_df = extractor.run()
            &gt;&gt;&gt; print(result_df.head())

            This will fetch and process assay data for &#39;CYP2D6&#39; and &#39;CYP3A4&#39;, returning a DataFrame with the processed data.
        &#34;&#34;&#34;

        # Initialize an empty list to store enzymes with successful responses
        enzymes_with_response = []

        # Keep a copy of the original list to identify removed enzymes later
        original_enzyme_list = self.enzyme_list.copy()

        for enzyme in self.enzyme_list:
            # Formulate the URL
            url = f&#34;{self._base_url}/{enzyme}/concise/CSV&#34;

            try:
                response = requests.get(url)
                # Check for a successful response (status code 200)
                if response.status_code == 200:
                    enzymes_with_response.append(enzyme)  # Keep the enzyme in the new list
            except requests.RequestException:
                # If there&#39;s an exception, skip adding the enzyme to the new list
                pass

        # Update the enzyme list with only the enzymes that had a successful response
        self.enzyme_list = enzymes_with_response

        # Identify and print the removed enzymes
        removed_enzymes = [enzyme for enzyme in original_enzyme_list if enzyme not in enzymes_with_response]
        if removed_enzymes:
            print(&#34;These enzymes were removed because their names aren&#39;t correct:&#34;, &#34;[&#34;, &#34;, &#34;.join(removed_enzymes), &#34;]&#34;)

        df_list = self._process_enzymes(self.enzyme_list)
        df = self._concatenate_data(df_list)
        substrings_to_filter = [&#39;CYP&#39;, &#39;Cytochrome&#39;]
        pattern = &#39;|&#39;.join(substrings_to_filter)
        df = df[df[&#39;Assay Name&#39;].str.contains(pattern, case=False, na=False)]
        df.to_csv(&#39;Data/AllDataConnected.csv&#39;, index=False)
        return df


    def _fetch_gene_details(self, gene_id):
        &#34;&#34;&#34;
        Fetches gene details in parallel using the PubChem API.

        Args:
            gene_id (int): The gene ID for fetching details.

        Returns:
            tuple: Contains gene ID, symbol, taxonomy, taxonomy ID, and synonyms.
        &#34;&#34;&#34;
        BASE_URL = &#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug&#34;
        url = f&#34;{BASE_URL}/gene/geneid/{int(gene_id)}/summary/JSON&#34;
        try:
            response = self._make_request(url)
            data = response.json()

            # Extracting the necessary details
            symbol = data[&#39;GeneSummaries&#39;][&#39;GeneSummary&#39;][0].get(&#39;Symbol&#39;, None)
            taxonomy = data[&#39;GeneSummaries&#39;][&#39;GeneSummary&#39;][0].get(&#39;Taxonomy&#39;, None)
            taxonomy_id = data[&#39;GeneSummaries&#39;][&#39;GeneSummary&#39;][0].get(&#39;TaxonomyID&#39;, None)
            synonyms = data[&#39;GeneSummaries&#39;][&#39;GeneSummary&#39;][0].get(&#39;Synonym&#39;, None)
            # print(type(synonyms))
            return gene_id, symbol, taxonomy, taxonomy_id, synonyms
        except Exception as e:
            logging.error(f&#34;Error fetching details for gene_id {gene_id}: {e}&#34;)
            return gene_id, None, None, None, None


    def extract_gene_properties(self, main_data):
        &#34;&#34;&#34;
        Extracts and processes gene properties from a given data source, specifically targeting genes
        relevant to the study (e.g., CYP enzymes) and records their details in a structured DataFrame.

        This method reads gene data from a CSV file specified by `main_data`, queries the PubChem database
        for additional properties of each unique gene ID found in the file, and compiles these properties
        into a new DataFrame. It focuses on fetching details like gene symbols, taxonomy, taxonomy IDs, and
        synonyms for each gene. The final DataFrame is filtered to include only genes of particular interest
        (e.g., certain CYP enzymes) and saved to a separate CSV file for further analysis or use.

        Parameters:
            main_data (str): Path to a CSV file containing main data was which generated after running
                            `extractor.run()`.

        Returns:
            pd.DataFrame: A DataFrame containing the compiled gene properties, including GeneID, Symbol,
                        Taxonomy, Taxonomy ID, and Synonyms, filtered to include only specified genes
                        of interest. This DataFrame is also saved to &#39;Data/Nodes/Gene_Properties.csv&#39;.

        Raises:
            Exception: If there&#39;s an issue reading the initial CSV file or fetching gene details from PubChem,
                    details of the exception are logged, and the method proceeds to process the next gene ID.

        Example:
            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; extractor.run()
            &gt;&gt;&gt; gene_properties_df = extractor.extract_gene_properties(&#39;Data/AllDataConnected.csv&#39;)
            &gt;&gt;&gt; print(gene_properties_df.head())

            This would read gene IDs from &#39;Data/AllDataConnected.csv&#39;, fetch their properties from PubChem,
            and compile the details into a DataFrame, filtering for specified genes of interest and saving
            the results to &#39;Data/Nodes/Gene_Properties.csv&#39;.

        Note:
            The method filters the resulting DataFrame to include only genes with symbols in the predefined
            enzyme_list. Adjust this list as necessary to match the focus of your study or application.
        &#34;&#34;&#34;
        df = pd.read_csv(main_data)
        df_gene = pd.DataFrame(columns=[&#39;GeneID&#39;, &#39;Symbol&#39;, &#39;Taxonomy&#39;, &#39;Taxonomy ID&#39;, &#39;Synonyms&#39;])

        unique_gene_ids = df[&#39;Target GeneID&#39;].unique().tolist()

        gene_details = []

        for gene_id in unique_gene_ids:
            try:
                gene_id, symbol, taxonomy, taxonomy_id, synonyms = self._fetch_gene_details(gene_id)
                gene_details.append({
                    &#39;GeneID&#39;: gene_id,
                    &#39;Symbol&#39;: symbol,
                    &#39;Taxonomy&#39;: taxonomy,
                    &#39;Taxonomy ID&#39;: taxonomy_id,
                    &#39;Synonyms&#39;: str(synonyms)
                })
            except Exception as exc:
                logging.error(f&#34;Error occurred while processing gene_id {gene_id}: {exc}&#34;)
                gene_details.append({
                    &#39;GeneID&#39;: gene_id,
                    &#39;Symbol&#39;: None,
                    &#39;Taxonomy&#39;: None,
                    &#39;Taxonomy ID&#39;: None,
                    &#39;Synonyms&#39;: None
                })

        # Now create the DataFrame from the list of dictionaries
        df_gene = pd.DataFrame(gene_details)
        n = self._enzyme_count
        gene_ids = df[&#39;Target GeneID&#39;].value_counts().head(n).index.tolist()
        df_gene = df_gene[df_gene[&#39;GeneID&#39;].isin([int(item) for item in gene_ids])]
        df_gene.to_csv(&#39;Data/Nodes/Gene_Properties.csv&#39;, sep=&#39;,&#39;, index=False)
        return df_gene


    def _fetch_assay_details(self, aid):
        &#34;&#34;&#34;
        Fetches assay details from the PubChem API for a given assay ID.

        Args:
            aid (int): The assay ID to fetch details for.

        Returns:
            dict: A dictionary containing assay details like AID, SourceName, SourceID, Name, and Description.
                  Returns None if an error occurs during fetching or parsing.
        &#34;&#34;&#34;
        BASE_URL = &#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug&#34;
        url = f&#34;{BASE_URL}/assay/aid/{aid}/summary/XML&#34;  # Constructing the API URL
        response = self._make_request(url)  # Making the API request
        xml_data = response.text  # Getting the response text

        try:
            # Parsing the XML response
            data_dict = xmltodict.parse(xml_data)
            properties = [&#39;AID&#39;, &#39;SourceName&#39;, &#39;SourceID&#39;, &#39;Name&#39;, &#39;Description&#39;]
            assay_data = {}
            # Extracting required properties from the parsed XML
            for prop in properties:
                assay_data[prop] = data_dict.get(&#39;AssaySummaries&#39;, {}).get(&#39;AssaySummary&#39;, {}).get(prop, None)
            return assay_data
        except Exception as e:
            logging.error(f&#34;Error parsing XML for AID {aid}: {e}&#34;)
            return None


    def extract_assay_properties(self, main_data):
        &#34;&#34;&#34;
        Extracts detailed properties of assays from PubChem for each unique assay ID found in the input data file.

        This method processes an input CSV file containing assay IDs (AID) and performs concurrent HTTP requests to
        fetch detailed assay properties from the PubChem database. The retrieved details include assay type, activity name,
        source name, source ID, name, and description. These properties are compiled into a new DataFrame, which is then
        saved to a CSV file for further analysis or use.

        The method employs a ThreadPoolExecutor to manage concurrent requests efficiently, improving the performance
        when dealing with a large number of assay IDs. Errors encountered during data fetching are logged, and the
        process continues with the next assay ID, ensuring the method&#39;s robustness.

        Parameters:
            main_data (str): Path to a CSV file containing main data was which generated after running
                            `extractor.run()`.

        Returns:
            pd.DataFrame: A DataFrame containing the fetched assay properties, including columns for AID, Assay Type,
                        Activity Name, SourceName, SourceID, Name, and Description. This DataFrame is saved to
                        &#39;Data/Nodes/Assay_Properties.csv&#39; in the current working directory.

        Raises:
            ValueError: If the input CSV file is empty or does not contain the &#39;AID&#39; column.

        Example:
            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; extractor.run()
            &gt;&gt;&gt; assay_properties_df = extractor.extract_assay_properties(&#39;Data/AllDataConnected.csv&#39;)
            &gt;&gt;&gt; print(assay_properties_df.head())

            This example reads assay IDs from &#39;Data/AllDataConnected.csv&#39;, queries PubChem for their detailed properties,
            and compiles the results into a DataFrame, which is also saved to &#39;Data/Nodes/Assay_Properties.csv&#39;.

        Note:
            This method requires network access to the PubChem API and assumes the availability of a valid &#39;AID&#39; column
            in the input CSV file. Ensure the input file path is correct and accessible to avoid errors during processing.
        &#34;&#34;&#34;

        df = pd.read_csv(main_data)

        # Check if the DataFrame is valid
        if df.empty or &#39;AID&#39; not in df.columns:
            logging.error(&#34;DataFrame is empty or does not contain &#39;AID&#39; column.&#34;)
            return pd.DataFrame()

        unique_aids = df[&#39;AID&#39;].unique().tolist()  # Extracting unique assay IDs

        columns = [&#39;AID&#39;, &#39;Assay Type&#39;, &#39;Activity Name&#39;, &#39;SourceName&#39;, &#39;SourceID&#39;, &#39;Name&#39;, &#39;Description&#39;]
        assay_df = pd.DataFrame(columns=columns)  # Initializing a DataFrame to store assay properties

        # Using ThreadPoolExecutor for concurrent fetching of assay details
        with concurrent.futures.ThreadPoolExecutor(max_workers=self._CONCURRENT_REQUEST_LIMIT) as executor:
            future_to_aid = {executor.submit(self._fetch_assay_details, aid): aid for aid in unique_aids}

            # Iterating over completed futures
            for future in concurrent.futures.as_completed(future_to_aid):
                aid = future_to_aid[future]
                try:
                    assay_data = future.result()  # Fetching the result from the future
                    if assay_data:
                        # Preparing a new row with the fetched data
                        new_row = {
                            &#39;AID&#39;: aid,
                            &#39;Assay Type&#39;: df.loc[df[&#39;AID&#39;] == aid, &#39;Assay Type&#39;].iloc[0],
                            &#39;Activity Name&#39;: df.loc[df[&#39;AID&#39;] == aid, &#39;Activity Name&#39;].iloc[0],
                            **assay_data
                        }
                        # Adding the new row to the DataFrame
                        assay_df = pd.concat([assay_df, pd.DataFrame([new_row])], ignore_index=True)
                except Exception as exc:
                    # Logging any errors encountered during the fetch
                    logging.error(f&#34;Error occurred while processing AID {aid}: {exc}&#34;)

        # Saving the updated DataFrame to a CSV file
        assay_df.to_csv(&#39;Data/Nodes/Assay_Properties.csv&#39;, sep=&#39;,&#39;, index=False)
        return assay_df


    def extract_protein_properties(self, main_data):
        &#34;&#34;&#34;
        Extracts and compiles protein properties from the NCBI protein database based on accession numbers.

        Given a CSV file specified by `main_data`, this method reads protein accession numbers and performs web
        scraping on the NCBI protein database pages to extract protein titles. The method constructs a URL for
        each accession number, sends a request to retrieve the page content, and parses the HTML to find the
        protein title. The extracted titles, along with their corresponding accession numbers and URLs, are
        compiled into a DataFrame. This DataFrame is saved to a CSV file, providing a structured summary of
        protein properties for further analysis or use.

        Parameters:
            main_data (str): Path to a CSV file containing main data was which generated after running
                            `extractor.run()`.

        Returns:
            pd.DataFrame: A DataFrame with columns &#39;RefSeq Accession&#39;, &#39;URL&#39;, and &#39;Description&#39;, where
                        &#39;Description&#39; contains the title of the protein extracted from its NCBI page.
                        This DataFrame is saved to &#39;Data/Nodes/Protein_Properties.csv&#39; in the current
                        working directory.

        Raises:
            Exception: If there&#39;s an issue reading the initial CSV file or querying the NCBI database,
                    details of the exception are logged. The method continues processing the next
                    accession number, ensuring robustness against individual failures.

        Example:
            Assuming &#39;protein_data.csv&#39; contains a column &#39;Target Accession&#39; with accession numbers:

            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; extractor.run() # you need to run this only once
            &gt;&gt;&gt; protein_properties_df = extractor.extract_protein_properties(&#39;Data/AllDataConnected.csv&#39;)
            &gt;&gt;&gt; print(protein_properties_df.head())

            This would read accession numbers from &#39;Data/AllDataConnected.csv&#39;, scrape their titles from the
            NCBI protein database, and compile the results into a DataFrame, which is also saved to
            &#39;Data/Nodes/Protein_Properties.csv&#39;.

        Note:
            This method requires internet access to query the NCBI protein database. Ensure the input file
            path is correct and accessible to avoid errors during processing. Web scraping is dependent on
            the structure of the web page; changes to the NCBI protein database pages may require updates
            to the scraping logic.
        &#34;&#34;&#34;

        # Initialize a list to store the extracted data
        data = []

        n = self._enzyme_count
        df = pd.read_csv(main_data)
        gene_ids = df[&#39;Target GeneID&#39;].value_counts().head(n).index.tolist()
        df = df[df[&#39;Target GeneID&#39;].isin([int(item) for item in gene_ids])]
        Accessions = df[&#39;Target Accession&#39;].unique().tolist()
        # Iterate over each protein accession number in the DataFrame
        for accession in Accessions:
            # Construct the URL to query the NCBI protein database
            url = f&#34;https://www.ncbi.nlm.nih.gov/protein/{accession}&#34;

            try:
                # Send an HTTP request to the URL
                response = requests.get(url)

                # Parse the HTML content of the response
                soup = BeautifulSoup(response.text, &#39;html.parser&#39;)

                # Extract the title from the parsed HTML
                title = soup.title.string if soup.title else &#39;Title Not Found&#39;

                # Append the extracted data to the list
                data.append({&#39;RefSeq Accession&#39;: accession, &#39;URL&#39;: url, &#39;Description&#39;: title})
            except Exception as e:
                # In case of an error, log the error message
                data.append({&#39;RefSeq Accession&#39;: accession, &#39;URL&#39;: url, &#39;Description&#39;: f&#39;Error: {e}&#39;})

        # Convert the list of data into a DataFrame
        protein_df = pd.DataFrame(data)

        # Save the DataFrame to a CSV file
        protein_df.to_csv(&#39;Data/Nodes/Protein_Properties.csv&#39;, sep=&#39;,&#39;, index=False)

        # Return the DataFrame
        return protein_df


    def fetch_data(self, cid):
        &#34;&#34;&#34;
        Retrieves detailed chemical compound properties for a specified Compound ID (CID) from the PubChem database.

        This method constructs a query URL to fetch a wide range of properties for the given CID from PubChem, including
        molecular formula, molecular weight, canonical and isomeric SMILES, InChI codes, physicochemical properties, and
        more. If the CID is valid and data is available, it returns a pandas DataFrame containing these properties. This
        method also generates a URL to retrieve the structure image of the compound as a 2D PNG image, adding it as a
        column in the DataFrame. In cases where the CID is NaN or an error occurs during data retrieval, an empty DataFrame
        is returned.

        Parameters:
            cid (int or float): The Compound ID for which to fetch data. Can be an integer or NaN.

        Returns:
            pd.DataFrame: A DataFrame containing the fetched properties for the given CID. The DataFrame includes
                        columns for each property fetched from PubChem, along with a &#39;StructureImage2DURL&#39; column
                        containing the URL to the compound&#39;s structure image. Returns an empty DataFrame if the CID
                        is NaN or if any error occurs during the fetch operation.

        Raises:
            Exception: Logs an error message if the request to PubChem fails or if the response cannot be processed
                    into a DataFrame.

        Example:
            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; compound_data_df = extractor.fetch_data(2244)
            &gt;&gt;&gt; print(compound_data_df.head())

            This example fetches the properties for the compound with CID 2244 from PubChem and prints the first few rows
            of the resulting DataFrame.

        Note:
            This method requires an active internet connection to access the PubChem database. Ensure that the CID provided
            is valid and not NaN to avoid fetching errors. The structure and availability of data fields are subject to the
            current state of the PubChem database and may vary.
        &#34;&#34;&#34;
        if pd.isna(cid):
            return pd.DataFrame()  # Return an empty DataFrame for NaN CIDs

        cid = int(cid)  # Convert CID to integer
        url = (f&#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/&#34;
               &#34;MolecularFormula,MolecularWeight,CanonicalSMILES,IsomericSMILES,InChI,&#34;
               &#34;InChIKey,IUPACName,Title,XLogP,ExactMass,MonoisotopicMass,TPSA,Complexity,&#34;
               &#34;Charge,HBondDonorCount,HBondAcceptorCount,RotatableBondCount,HeavyAtomCount,&#34;
               &#34;IsotopeAtomCount,AtomStereoCount,DefinedAtomStereoCount,UndefinedAtomStereoCount,&#34;
               &#34;BondStereoCount,DefinedBondStereoCount,UndefinedBondStereoCount,CovalentUnitCount,&#34;
               &#34;PatentCount,PatentFamilyCount,LiteratureCount,Volume3D,XStericQuadrupole3D,&#34;
               &#34;YStericQuadrupole3D,ZStericQuadrupole3D,FeatureCount3D,FeatureAcceptorCount3D,&#34;
               &#34;FeatureDonorCount3D,FeatureAnionCount3D,FeatureCationCount3D,FeatureRingCount3D,&#34;
               &#34;FeatureHydrophobeCount3D,ConformerModelRMSD3D,EffectiveRotorCount3D,ConformerCount3D,&#34;
               &#34;Fingerprint2D/CSV&#34;)
        try:
            response = requests.get(url)
            response.raise_for_status()
            compound_data = pd.read_csv(StringIO(response.text), sep=&#39;,&#39;, low_memory=False)
            compound_data[&#39;StructureImage2DURL&#39;] = f&#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/PNG&#34;
            return compound_data
        except Exception as e:
            logging.error(f&#34;Error processing CID {cid}: {e}&#34;)
            return pd.DataFrame()  # Return an empty DataFrame in case of error


    def extract_compound_properties(self, main_data):
        &#34;&#34;&#34;
        Extracts and aggregates compound properties from PubChem for a list of compounds associated with specific genes.

        This method processes a CSV file specified by `main_data`, which contains gene identifiers and their associated
        compound IDs (CIDs). It selects compounds related to the top `n` most frequently occurring genes in the dataset,
        where `n` is determined by the instance&#39;s `_enzyme_count` attribute. The method then fetches detailed compound
        properties from PubChem in chunks, using concurrent requests to improve efficiency and manage the load on the
        PubChem API. The fetched compound properties are aggregated into a single DataFrame and saved to multiple CSV files,
        one for each chunk of compound IDs processed.

        Parameters:
            main_data (str): Path to a CSV file containing main data was which generated after running
                            `extractor.run()`.

        Side Effects:
            - Saves the aggregated compound properties to CSV files in the current working directory. The files are named
            &#39;Data/Nodes/Compound_Properties/Chunk_{i}.csv&#39;, where `{i}` is the chunk index.

        Returns:
            None: This method does not return a value. Instead, it saves the fetched compound data directly to CSV files.

        Raises:
            Exception: Logs an error and continues processing the next CID if an error occurs while fetching data for a specific CID.

        Example:
            &gt;&gt;&gt; extractor = NodePropertiesExtractor([&#39;CYP2D6&#39;, &#39;CYP3A4&#39;])
            &gt;&gt;&gt; extractor.create_data_directories()
            &gt;&gt;&gt; extractor.extract_compound_properties(&#39;Data/AllDataConnected.csv&#39;)
            This will read &#39;Data/AllDataConnected.csv&#39;, filter for compounds associated with the top n genes, fetch their properties
            from PubChem, and save the results into multiple CSV files for each chunk of compounds processed.

        Note:
            - Ensure that the &#39;main_data&#39; CSV file exists and is accessible at the specified path.
            - The method automatically handles NaN values in the &#39;CID&#39; column and excludes them from processing.
            - The `enzyme_count` attribute determines the number of top genes for which compound properties will be fetched.
            - Internet access is required to fetch compound data from the PubChem API.
            - The method employs a `ThreadPoolExecutor` with a configurable number of workers (default is len(enzyme_list)) to parallelize
            requests, which can be adjusted based on system capabilities and API rate limits.
        &#34;&#34;&#34;

        n = self._enzyme_count
        df = pd.read_csv(main_data)
        gene_ids = df[&#39;Target GeneID&#39;].value_counts().head(n).index.tolist()
        df = df[df[&#39;Target GeneID&#39;].isin([int(item) for item in gene_ids])]
        df = df.dropna(subset=[&#39;CID&#39;])
        IDs = df[&#39;CID&#39;].unique().tolist()

        # Define chunk size and calculate number of chunks
        chunk_size = 10000
        num_chunks = math.ceil(len(IDs) / chunk_size)

        for i in range(num_chunks):
            # Calculate start and end indices for each chunk
            start_index = i * chunk_size
            end_index = start_index + chunk_size

            # Extract chunk of CIDs
            chunk_cids = IDs[start_index:end_index]
            # chunk_cids = [x for x in chunk_cids if not np.isnan(x)]

            # Use ThreadPoolExecutor to parallelize requests for the chunk
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_cid = {executor.submit(self.fetch_data, cid): cid for cid in chunk_cids}
                results = []

                for future in as_completed(future_to_cid):
                    cid = future_to_cid[future]
                    try:
                        data = future.result()
                        results.append(data)
                    except Exception as e:
                        logging.error(f&#34;Error processing CID {cid}: {e}&#34;)

            # Concatenate results for the current chunk
            chunk_df = pd.concat(results, ignore_index=True)

            # Save the concatenated DataFrame to a CSV file for the chunk
            chunk_df.to_csv(f&#39;Data/Nodes/Compound_Properties/Chunk_{i}.csv&#39;, sep=&#39;,&#39;, index=False)

        return</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.NodePropertiesExtractor.extract_assay_properties"><code class="name flex">
<span>def <span class="ident">extract_assay_properties</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts detailed properties of assays from PubChem for each unique assay ID found in the input data file.</p>
<p>This method processes an input CSV file containing assay IDs (AID) and performs concurrent HTTP requests to
fetch detailed assay properties from the PubChem database. The retrieved details include assay type, activity name,
source name, source ID, name, and description. These properties are compiled into a new DataFrame, which is then
saved to a CSV file for further analysis or use.</p>
<p>The method employs a ThreadPoolExecutor to manage concurrent requests efficiently, improving the performance
when dealing with a large number of assay IDs. Errors encountered during data fetching are logged, and the
process continues with the next assay ID, ensuring the method's robustness.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to a CSV file containing main data was which generated after running
<code>extractor.run()</code>.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the fetched assay properties, including columns for AID, Assay Type,
Activity Name, SourceName, SourceID, Name, and Description. This DataFrame is saved to
'Data/Nodes/Assay_Properties.csv' in the current working directory.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the input CSV file is empty or does not contain the 'AID' column.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; extractor.run()
&gt;&gt;&gt; assay_properties_df = extractor.extract_assay_properties('Data/AllDataConnected.csv')
&gt;&gt;&gt; print(assay_properties_df.head())
</code></pre>
<p>This example reads assay IDs from 'Data/AllDataConnected.csv', queries PubChem for their detailed properties,
and compiles the results into a DataFrame, which is also saved to 'Data/Nodes/Assay_Properties.csv'.</p>
<h2 id="note">Note</h2>
<p>This method requires network access to the PubChem API and assumes the availability of a valid 'AID' column
in the input CSV file. Ensure the input file path is correct and accessible to avoid errors during processing.</p></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.extract_compound_properties"><code class="name flex">
<span>def <span class="ident">extract_compound_properties</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and aggregates compound properties from PubChem for a list of compounds associated with specific genes.</p>
<p>This method processes a CSV file specified by <code>main_data</code>, which contains gene identifiers and their associated
compound IDs (CIDs). It selects compounds related to the top <code>n</code> most frequently occurring genes in the dataset,
where <code>n</code> is determined by the instance's <code>_enzyme_count</code> attribute. The method then fetches detailed compound
properties from PubChem in chunks, using concurrent requests to improve efficiency and manage the load on the
PubChem API. The fetched compound properties are aggregated into a single DataFrame and saved to multiple CSV files,
one for each chunk of compound IDs processed.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to a CSV file containing main data was which generated after running
<code>extractor.run()</code>.</p>
<p>Side Effects:
- Saves the aggregated compound properties to CSV files in the current working directory. The files are named
'Data/Nodes/Compound_Properties/Chunk_{i}.csv', where <code>{i}</code> is the chunk index.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>This method does not return a value. Instead, it saves the fetched compound data directly to CSV files.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>Logs an error and continues processing the next CID if an error occurs while fetching data for a specific CID.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; extractor.extract_compound_properties('Data/AllDataConnected.csv')
This will read 'Data/AllDataConnected.csv', filter for compounds associated with the top n genes, fetch their properties
from PubChem, and save the results into multiple CSV files for each chunk of compounds processed.
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Ensure that the 'main_data' CSV file exists and is accessible at the specified path.</li>
<li>The method automatically handles NaN values in the 'CID' column and excludes them from processing.</li>
<li>The <code>enzyme_count</code> attribute determines the number of top genes for which compound properties will be fetched.</li>
<li>Internet access is required to fetch compound data from the PubChem API.</li>
<li>The method employs a <code>ThreadPoolExecutor</code> with a configurable number of workers (default is len(enzyme_list)) to parallelize
requests, which can be adjusted based on system capabilities and API rate limits.</li>
</ul></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.extract_gene_properties"><code class="name flex">
<span>def <span class="ident">extract_gene_properties</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and processes gene properties from a given data source, specifically targeting genes
relevant to the study (e.g., CYP enzymes) and records their details in a structured DataFrame.</p>
<p>This method reads gene data from a CSV file specified by <code>main_data</code>, queries the PubChem database
for additional properties of each unique gene ID found in the file, and compiles these properties
into a new DataFrame. It focuses on fetching details like gene symbols, taxonomy, taxonomy IDs, and
synonyms for each gene. The final DataFrame is filtered to include only genes of particular interest
(e.g., certain CYP enzymes) and saved to a separate CSV file for further analysis or use.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to a CSV file containing main data was which generated after running
<code>extractor.run()</code>.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the compiled gene properties, including GeneID, Symbol,
Taxonomy, Taxonomy ID, and Synonyms, filtered to include only specified genes
of interest. This DataFrame is also saved to 'Data/Nodes/Gene_Properties.csv'.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If there's an issue reading the initial CSV file or fetching gene details from PubChem,
details of the exception are logged, and the method proceeds to process the next gene ID.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; extractor.run()
&gt;&gt;&gt; gene_properties_df = extractor.extract_gene_properties('Data/AllDataConnected.csv')
&gt;&gt;&gt; print(gene_properties_df.head())
</code></pre>
<p>This would read gene IDs from 'Data/AllDataConnected.csv', fetch their properties from PubChem,
and compile the details into a DataFrame, filtering for specified genes of interest and saving
the results to 'Data/Nodes/Gene_Properties.csv'.</p>
<h2 id="note">Note</h2>
<p>The method filters the resulting DataFrame to include only genes with symbols in the predefined
enzyme_list. Adjust this list as necessary to match the focus of your study or application.</p></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.extract_protein_properties"><code class="name flex">
<span>def <span class="ident">extract_protein_properties</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and compiles protein properties from the NCBI protein database based on accession numbers.</p>
<p>Given a CSV file specified by <code>main_data</code>, this method reads protein accession numbers and performs web
scraping on the NCBI protein database pages to extract protein titles. The method constructs a URL for
each accession number, sends a request to retrieve the page content, and parses the HTML to find the
protein title. The extracted titles, along with their corresponding accession numbers and URLs, are
compiled into a DataFrame. This DataFrame is saved to a CSV file, providing a structured summary of
protein properties for further analysis or use.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to a CSV file containing main data was which generated after running
<code>extractor.run()</code>.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame with columns 'RefSeq Accession', 'URL', and 'Description', where
'Description' contains the title of the protein extracted from its NCBI page.
This DataFrame is saved to 'Data/Nodes/Protein_Properties.csv' in the current
working directory.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If there's an issue reading the initial CSV file or querying the NCBI database,
details of the exception are logged. The method continues processing the next
accession number, ensuring robustness against individual failures.</dd>
</dl>
<h2 id="example">Example</h2>
<p>Assuming 'protein_data.csv' contains a column 'Target Accession' with accession numbers:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; extractor.run() # you need to run this only once
&gt;&gt;&gt; protein_properties_df = extractor.extract_protein_properties('Data/AllDataConnected.csv')
&gt;&gt;&gt; print(protein_properties_df.head())
</code></pre>
<p>This would read accession numbers from 'Data/AllDataConnected.csv', scrape their titles from the
NCBI protein database, and compile the results into a DataFrame, which is also saved to
'Data/Nodes/Protein_Properties.csv'.</p>
<h2 id="note">Note</h2>
<p>This method requires internet access to query the NCBI protein database. Ensure the input file
path is correct and accessible to avoid errors during processing. Web scraping is dependent on
the structure of the web page; changes to the NCBI protein database pages may require updates
to the scraping logic.</p></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.fetch_data"><code class="name flex">
<span>def <span class="ident">fetch_data</span></span>(<span>self, cid)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves detailed chemical compound properties for a specified Compound ID (CID) from the PubChem database.</p>
<p>This method constructs a query URL to fetch a wide range of properties for the given CID from PubChem, including
molecular formula, molecular weight, canonical and isomeric SMILES, InChI codes, physicochemical properties, and
more. If the CID is valid and data is available, it returns a pandas DataFrame containing these properties. This
method also generates a URL to retrieve the structure image of the compound as a 2D PNG image, adding it as a
column in the DataFrame. In cases where the CID is NaN or an error occurs during data retrieval, an empty DataFrame
is returned.</p>
<h2 id="parameters">Parameters</h2>
<p>cid (int or float): The Compound ID for which to fetch data. Can be an integer or NaN.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the fetched properties for the given CID. The DataFrame includes
columns for each property fetched from PubChem, along with a 'StructureImage2DURL' column
containing the URL to the compound's structure image. Returns an empty DataFrame if the CID
is NaN or if any error occurs during the fetch operation.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>Logs an error message if the request to PubChem fails or if the response cannot be processed
into a DataFrame.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; compound_data_df = extractor.fetch_data(2244)
&gt;&gt;&gt; print(compound_data_df.head())
</code></pre>
<p>This example fetches the properties for the compound with CID 2244 from PubChem and prints the first few rows
of the resulting DataFrame.</p>
<h2 id="note">Note</h2>
<p>This method requires an active internet connection to access the PubChem database. Ensure that the CID provided
is valid and not NaN to avoid fetching errors. The structure and availability of data fields are subject to the
current state of the PubChem database and may vary.</p></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.get_enzyme_assays"><code class="name flex">
<span>def <span class="ident">get_enzyme_assays</span></span>(<span>self, enzyme)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches assay data for a specified enzyme from the PubChem database and returns it as a pandas DataFrame.</p>
<p>This method constructs a URL to query the PubChem database for concise assay data related to the given enzyme.
It processes the CSV response into a DataFrame, which includes various assay data points provided by PubChem.</p>
<h2 id="parameters">Parameters</h2>
<p>enzyme (str): The name of the enzyme for which assay data is requested. This name is used in the API query.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the assay data fetched from PubChem for the specified enzyme. The DataFrame
includes columns based on the CSV response from PubChem, such as assay ID, results, and conditions.
Returns None if no data is available or if an error occurs during data fetching or processing.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>requests.RequestException</code></dt>
<dd>If an error occurs during the HTTP request to the PubChem API.</dd>
<dt><code>pd.errors.EmptyDataError</code></dt>
<dd>If the response from PubChem contains no data.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['enzyme'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; enzyme_assays_df = extractor.get_enzyme_assays('enzyme')
&gt;&gt;&gt; print(enzyme_assays_df.head())
</code></pre></div>
</dd>
<dt id="chemgraphbuilder.NodePropertiesExtractor.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Orchestrates the process of fetching, filtering, and aggregating assay data from PubChem for a predefined list of enzymes.</p>
<p>This method iteratively queries PubChem for assay data corresponding to each enzyme specified in the <code>enzyme_list</code> attribute during class initialization. It performs the following steps for each enzyme:
1. Constructs a query URL and fetches assay data from PubChem.
2. Filters the fetched data based on predefined criteria (e.g., containing specific substrings in the assay name).
3. Aggregates the filtered data into a single pandas DataFrame.
4. Identifies enzymes for which data could not be fetched or were excluded based on filtering criteria, logging their names.</p>
<p>The final aggregated DataFrame, containing assay data for all successfully processed enzymes, is then saved to a CSV file. This method facilitates the extraction and preprocessing of chemical assay data for further analysis or integration into knowledge graphs.</p>
<h2 id="note">Note</h2>
<ul>
<li>This method relies on the successful response from PubChem for each enzyme query.</li>
<li>Enzymes with no available data or failing to meet the filtering criteria are excluded from the final DataFrame.</li>
<li>The output CSV file is saved in the current working directory with the name 'Data/AllDataConnected.csv'.</li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing the aggregated and filtered assay data for the specified enzymes.
Columns in the DataFrame correspond to the assay data fields returned by PubChem, subject to
the filtering criteria applied within this method.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>requests.RequestException</code></dt>
<dd>If there is an issue with fetching data from PubChem, such as a network problem or
an invalid response.</dd>
</dl>
<h2 id="example">Example</h2>
<p>Assuming <code>enzyme_list</code> was set to ['CYP2D6', 'CYP3A4'] during class initialization:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])
&gt;&gt;&gt; extractor.create_data_directories()
&gt;&gt;&gt; result_df = extractor.run()
&gt;&gt;&gt; print(result_df.head())
</code></pre>
<p>This will fetch and process assay data for 'CYP2D6' and 'CYP3A4', returning a DataFrame with the processed data.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.RelationshipCollectorProcessor"><code class="flex name class">
<span>class <span class="ident">RelationshipCollectorProcessor</span></span>
<span>(</span><span>relationship_type)</span>
</code></dt>
<dd>
<div class="desc"><p>A class to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and RelationshipDataProcessor.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>relationship_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of relationship to collect data for.</dd>
<dt><strong><code>data_file</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the data file containing relationship data.</dd>
<dt><strong><code>extractor</code></strong> :&ensp;<code><a title="chemgraphbuilder.RelationshipPropertiesExtractor" href="#chemgraphbuilder.RelationshipPropertiesExtractor">RelationshipPropertiesExtractor</a></code></dt>
<dd>An instance of RelationshipPropertiesExtractor.</dd>
<dt><strong><code>processor</code></strong> :&ensp;<code><a title="chemgraphbuilder.RelationshipDataProcessor" href="#chemgraphbuilder.RelationshipDataProcessor">RelationshipDataProcessor</a></code></dt>
<dd>An instance of RelationshipDataProcessor.</dd>
</dl>
<p>Initializes the RelationshipCollectorProcessor with the relationship type and data file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>relationship_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of relationship to collect data for (e.g., 'Assay_Compound', 'Assay_Enzyme', 'Gene_Enzyme', 'Compound_Enzyme', 'Compound_Similarity', 'Compound_Cooccurrence', 'Compound_Transformation').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RelationshipCollectorProcessor:
    &#34;&#34;&#34;
    A class to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and RelationshipDataProcessor.

    Attributes:
        relationship_type (str): The type of relationship to collect data for.
        data_file (str): The path to the data file containing relationship data.
        extractor (RelationshipPropertiesExtractor): An instance of RelationshipPropertiesExtractor.
        processor (RelationshipDataProcessor): An instance of RelationshipDataProcessor.
    &#34;&#34;&#34;

    def __init__(self, relationship_type):
        &#34;&#34;&#34;
        Initializes the RelationshipCollectorProcessor with the relationship type and data file.

        Args:
            relationship_type (str): The type of relationship to collect data for (e.g., &#39;Assay_Compound&#39;, &#39;Assay_Enzyme&#39;, &#39;Gene_Enzyme&#39;, &#39;Compound_Enzyme&#39;, &#39;Compound_Similarity&#39;, &#39;Compound_Cooccurrence&#39;, &#39;Compound_Transformation&#39;).
        &#34;&#34;&#34;
        self.relationship_type = relationship_type
        self.data_file = &#34;Data/AllDataConnected.csv&#34;
        self.extractor = RelationshipPropertiesExtractor()
        self.processor = RelationshipDataProcessor(path=&#34;Data/Relationships/Assay_Compound_Relationship&#34;)

        # Setup data folder
        data_folder_setup = SetupDataFolder()
        data_folder_setup.setup()

    def collect_relationship_data(self):
        &#34;&#34;&#34;
        Collects and processes relationship data based on the relationship type and saves it to the appropriate file.
        &#34;&#34;&#34;
        if self.relationship_type == &#39;Assay_Compound&#39;:
            self.extractor.assay_compound_relationship(self.data_file)
            self.processor.process_files()
        elif self.relationship_type == &#39;Assay_Enzyme&#39;:
            self.extractor.assay_enzyme_relationship(self.data_file)
        elif self.relationship_type == &#39;Gene_Enzyme&#39;:
            self.extractor.gene_enzyme_relationship(self.data_file)
        elif self.relationship_type == &#39;Compound_Gene&#39;:
            self.extractor.compound_gene_relationship(self.data_file)
        elif self.relationship_type == &#39;Compound_Similarity&#39;:
            self.extractor.compound_similarity_relationship(self.data_file)
        elif self.relationship_type == &#39;Compound_Cooccurrence&#39;:
            self.extractor.compound_cooccurrence(self.data_file)
        elif self.relationship_type == &#39;Compound_Transformation&#39;:
            self.extractor.compound_transformation(self.data_file)
        else:
            logging.error(f&#34;Unsupported relationship type: {self.relationship_type}&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.RelationshipCollectorProcessor.collect_relationship_data"><code class="name flex">
<span>def <span class="ident">collect_relationship_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Collects and processes relationship data based on the relationship type and saves it to the appropriate file.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.RelationshipDataProcessor"><code class="flex name class">
<span>class <span class="ident">RelationshipDataProcessor</span></span>
<span>(</span><span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>A class to process relationship data from multiple CSV files, clean the data,
merge it into a single CSV file, and perform additional data processing.</p>
<p>Attributes:
path (str): The path to the directory containing the CSV files.
output_files (dict): A dictionary to store unique headers and corresponding output file paths.
csv_files (list): A list of all CSV files to be processed.
processed_csv_files (list): A list of processed CSV files.</p>
<p>Initialize the RelationshipDataProcessor with the specified path.</p>
<p>Parameters:
path (str): The path to the directory containing the CSV files.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RelationshipDataProcessor:
    &#34;&#34;&#34;
    A class to process relationship data from multiple CSV files, clean the data,
    merge it into a single CSV file, and perform additional data processing.

    Attributes:
    path (str): The path to the directory containing the CSV files.
    output_files (dict): A dictionary to store unique headers and corresponding output file paths.
    csv_files (list): A list of all CSV files to be processed.
    processed_csv_files (list): A list of processed CSV files.
    &#34;&#34;&#34;

    def __init__(self, path):
        &#34;&#34;&#34;
        Initialize the RelationshipDataProcessor with the specified path.

        Parameters:
        path (str): The path to the directory containing the CSV files.
        &#34;&#34;&#34;
        self.path = path
        self.output_files = {}
        self.csv_files = glob.glob(os.path.join(path, &#34;AID_*.csv&#34;))
        self.processed_csv_files = glob.glob(os.path.join(path, &#34;Assay_Compound_Relationship*.csv&#34;))

    def process_files(self):
        &#34;&#34;&#34;
        Process all CSV files by reading, cleaning, merging, and formatting the data.
        &#34;&#34;&#34;
        self._process_individual_files()
        print(&#34;Step 1: Individual files processed and cleaned.&#34;)

        self._filter_and_combine_data()
        print(&#34;Step 2: Data filtered and combined successfully.&#34;)

        self._clean_and_save_final_data()
        print(&#34;Step 3: Final data processing and merging completed.&#34;)

    def _process_individual_files(self):
        &#34;&#34;&#34;
        Process all individual CSV files: read, clean, and save them based on their headers.
        &#34;&#34;&#34;
        for file in self.csv_files:
            with open(file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
                header = next(f).strip()
                num_columns = len(header.split(&#39;,&#39;))

                if header not in self.output_files:
                    output_filename = f&#34;{self.path}/Assay_Compound_Relationship{len(self.output_files)}_cols-{num_columns}.csv&#34;
                    self.output_files[header] = output_filename
                    with open(output_filename, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f_out:
                        f_out.write(header + &#39;\n&#39;)

            df = pd.read_csv(file)
            df = df[header.split(&#39;,&#39;)]
            df.dropna(subset=[&#39;CID&#39;], inplace=True)
            df.dropna(axis=1, how=&#39;all&#39;, inplace=True)
            df.to_csv(self.output_files[header], mode=&#39;a&#39;, index=False, header=False)

    def _filter_and_combine_data(self):
        &#34;&#34;&#34;
        Filter and format the merged data by selecting specific columns and saving it to a final output file.
        &#34;&#34;&#34;

        output_file = os.path.join(self.path, &#39;Assay_Compound_DataSet.csv&#39;)

        unique_column_names = self._get_filtered_columns()

        # Create and write the header of the output file if not exists
        file_exists = os.path.isfile(output_file)
        if file_exists:
            os.remove(output_file)
        pd.DataFrame(columns=unique_column_names).to_csv(output_file, index=False)

        # Read each file, adjust columns, and append to the CSV
        for file in self.processed_csv_files:
            try:
                # Load just the column names first
                df_headers = pd.read_csv(file, nrows=0)
                df_headers.columns = [col.lower() for col in df_headers.columns]

                # Check and handle duplicate column names
                if df_headers.columns.duplicated().any():
                    df_headers = df_headers.loc[:, ~df_headers.columns.duplicated()]
                    print(f&#34;Duplicated columns removed from {file}&#34;)

                # Load the data in chunks using the cleaned column names
                for df in pd.read_csv(file, chunksize=10000, names=df_headers.columns, header=0):
                    # Reindex to the union of columns, filling missing ones with NaNs
                    df = df.reindex(columns=unique_column_names)
                    # Append to the output file
                    df.to_csv(output_file, mode=&#39;a&#39;, header=False, index=False)

                print(f&#34;Successfully processed and appended data from {file}&#34;)
            except Exception as e:
                print(f&#34;Error processing file {file}: {e}&#34;)

        print(f&#34;Data appended successfully to {output_file}&#34;)

    def _get_filtered_columns(self):
        &#34;&#34;&#34;
        Determine the columns to be included in the final output file based on specific keywords.

        Returns:
        list: A list of filtered columns.
        &#34;&#34;&#34;
        all_columns = set()
        for file in self.processed_csv_files:
            df = pd.read_csv(file, nrows=0)
            df.columns = [col.lower() for col in df.columns]
            all_columns.update(df.columns)

        keywords = [&#39;id&#39;, &#39;outcome&#39;, &#39;phenotype&#39;, &#39;activity_url&#39;]
        filtered_columns = sorted([col for col in all_columns if any(keyword in col for keyword in keywords)])
        print(f&#34;Filtered columns: {filtered_columns}&#34;)
        return filtered_columns

    def _get_filtered_columns(self):
        &#34;&#34;&#34;
        Determine the columns to be included in the final output file based on specific keywords.

        Returns:
        list: A list of filtered columns.
        &#34;&#34;&#34;
        all_columns = set()
        for file in self.processed_csv_files:
            df = pd.read_csv(file, nrows=0)
            df.columns = [col.lower() for col in df.columns]
            all_columns.update(df.columns)

        keywords = [&#39;id&#39;, &#39;outcome&#39;, &#39;phenotype&#39;, &#39;activity_url&#39;,&#39;activity direction&#39;]
        filtered_columns = sorted([col for col in all_columns if any(keyword in col for keyword in keywords)])
        print(f&#34;Filtered columns: {filtered_columns}&#34;)
        return filtered_columns

    def _clean_and_save_final_data(self):
        &#34;&#34;&#34;
        Clean and save the processed dataset.
        &#34;&#34;&#34;
        final_file_path = os.path.join(self.path, &#39;Assay_Compound_DataSet.csv&#39;)
        if os.path.exists(final_file_path):
            dataset = pd.read_csv(final_file_path)
            # print(dataset.columns)
            dataset.dropna(subset=[&#39;aid&#39;, &#39;cid&#39;], how=&#39;any&#39;, axis=0, inplace=True)

            phenotype_cols = [col for col in dataset.columns if col.startswith(&#39;phenotype&#39;)]
            dataset[&#39;measured_activity&#39;] = dataset[phenotype_cols].apply(self.most_frequent, axis=1)
            dataset = dataset.drop(columns=phenotype_cols)
            dataset.rename(columns={
                &#39;activity_outcome&#39;: &#39;Activity Outcome&#39;,
                &#39;activity_url&#39;: &#39;Activity URL&#39;,
                &#39;measured_activity&#39;: &#39;Phenotype&#39;,
                &#39;activity direction&#39;: &#39;Activity Direction&#39;,
                &#39;aid&#39;: &#39;AID&#39;,
                &#39;cid&#39;: &#39;CID&#39;
            }, inplace=True)

            AllDataConnected = pd.read_csv(&#39;Data/AllDataConnected.csv&#39;)

            # Process the dataset
            merged_df = dataset.merge(AllDataConnected, on=[&#39;AID&#39;, &#39;CID&#39;, &#39;Activity Outcome&#39;], how=&#39;left&#39;)
            merged_df.drop([&#39;PubMed ID&#39;, &#39;RNAi&#39;], axis=1, inplace=True)
            merged_df.dropna(axis=1, how=&#39;all&#39;, inplace=True)
            merged_df = merged_df.groupby([&#39;Activity Outcome&#39;, &#39;Assay Name&#39;]).apply(self.propagate_phenotype).reset_index(drop=True)
            merged_df = merged_df[merged_df[&#39;Target GeneID&#39;].isin([1576, 1565, 1559, 1557, 1544])]
            merged_df[&#39;AID&#39;] = merged_df[&#39;AID&#39;].astype(int)
            merged_df[&#39;CID&#39;] = merged_df[&#39;CID&#39;].astype(int)
            merged_df[&#39;Target GeneID&#39;] = merged_df[&#39;Target GeneID&#39;].astype(int)
            merged_df[&#39;Activity URL&#39;] = merged_df.apply(lambda row: f&#34;https://pubchem.ncbi.nlm.nih.gov/bioassay/{row[&#39;AID&#39;]}#sid={row[&#39;SID&#39;]}&#34;, axis=1)
            # Additional processing for determining labels and activity
            merged_df = self._determine_labels_and_activity(merged_df)
            Assay_Cpd = merged_df[[&#39;AID&#39;, &#39;CID&#39;, &#39;Activity Outcome&#39;, &#39;Activity URL&#39;,
                                   &#39;Activity Direction&#39;, &#39;Phenotype&#39;,
                                   &#39;Activity Value [uM]&#39;, &#39;Activity Name&#39;]]
            Assay_Cpd.to_csv(&#39;Data/Relationships/Assay_Compound_Relationship.csv&#39;, index=False)

            Cpd_Enzyme = merged_df[[&#39;CID&#39;, &#39;Target GeneID&#39;, &#39;Activity&#39;, &#39;AID&#39;]]
            Cpd_Enzyme.to_csv(&#39;Data/Relationships/Compound_Gene_Relationship.csv&#39;, index=False)
        else:
            print(f&#34;File {final_file_path} does not exist. No data to process.&#34;)

    @staticmethod
    def most_frequent(row):
        &#34;&#34;&#34;
        Find the most frequent string value in a row, excluding NaNs and non-string types.

        Parameters:
        row (Series): The row of data.

        Returns:
        str or None: The most frequent string value, or None if no strings are found.
        &#34;&#34;&#34;
        values = row.dropna()
        string_values = values[values.apply(lambda x: isinstance(x, str))]
        return string_values.mode()[0] if not string_values.empty else None

    @staticmethod
    def propagate_phenotype(group):
        &#34;&#34;&#34;
        Propagate non-null values within each group for the &#39;Phenotype&#39; column.

        Parameters:
        group (DataFrame): The group of data.

        Returns:
        DataFrame: The group with propagated &#39;Phenotype&#39; values.
        &#34;&#34;&#34;
        phenotype_value = group[&#39;Phenotype&#39;].dropna().unique()
        if len(phenotype_value) &gt; 0:
            group[&#39;Phenotype&#39;] = phenotype_value[0]
        return group

    def _determine_labels_and_activity(self, merged_df):
        &#34;&#34;&#34;
        Determine the labels and activity based on keywords in the assay names.
        &#34;&#34;&#34;

        # Define the mapping values
        inhibitor_keywords = [
            &#39;inhibition&#39;, &#39;reversible inhibition&#39;, &#39;time dependent inhibition&#39;,
            &#39;inhibitory activity&#39;, &#39;time-dependent inhibition&#39;, &#39;time dependent irreversible inhibition&#39;,
            &#39;inhibitory concentration&#39;, &#39;inhibitory effect&#39;, &#39;inhibitory potency&#39;,
            &#39;concentration required to inhibit&#39;, &#39;competitive inhibition&#39;, &#39;cyp inhibition&#39;,
            &#39;irreversible inhibition&#39;, &#39;mechanism based inhibition&#39;, &#39;mixed inhibition&#39;,
            &#39;mixed type inhibition&#39;, &#39;inhibitory constant&#39;, &#39;antagonistic activity&#39;, &#39;selectivity&#39;,
            &#39;s1p4 agonists&#39;, &#39;small molecule antagonists&#39;, &#39;displacement&#39;, &#39;mediated midazolam 1-hydroxylation&#39;,
            &#39;time/nadph-dependent inhibition&#39;, &#39;reversal inhibition&#39;, &#39;mechanism-based inhibition&#39;,
            &#39;mechanism based time dependent inhibition&#39;, &#39;reversible competitive inhibition&#39;,
            &#39;predictive competitive inhibition&#39;,&#39;noncompetitive inhibition&#39;, &#39;in vitro inhibitory&#39;,
            &#39;in vitro inhibition&#39;, &#39;inhibition of&#39;, &#39;direct inhibition&#39;,&#39;enzyme inhibition&#39;, &#39;dndi&#39;,
            &#39;inhibition assay&#39;
        ]

        ligand_keywords = [
            &#39;binding affinity&#39;, &#39;spectral binding&#39;, &#39;interaction with&#39;, &#39;bind&#39;,
            &#39;covalent binding affinity&#39;, &#39;apparent binding affinity&#39;
        ]

        inhibitor_substrate_keywords = [
            &#39;inhibitors and substrates&#39;
        ]

        inhibitor_activator_modulator_keywords = [
            &#39;apoprotein formation&#39;, &#39;panel assay&#39;, &#39;eurofins-panlabs enzyme assay&#39;
        ]

        substrate_keywords = [
            &#39;drug metabolism&#39;, &#39;prodrug&#39;, &#39;metabolic&#39;, &#39;oxidation&#39;, &#39;substrate activity&#39;,
            &#39;michaelis-menten&#39;, &#39;metabolic stability&#39;, &#39;bioactivation&#39;, &#39;drug level&#39;,
            &#39;enzyme-mediated drug depletion&#39;, &#39;enzyme-mediated compound formation&#39;,
            &#39;phenotyping&#39;, &#39;activity of human recombinant cyp&#39;, &#39;activity of recombinant cyp&#39;,
            &#39;activity at cyp&#39;, &#39;enzyme-mediated drug metabolism&#39;
        ]

        inactivator_keywords = [
            &#39;inactivator&#39;, &#39;inactivation of&#39;, &#39;mechanism based inactivation of&#39;, &#39;inactivators&#39;,
            &#39;metabolism dependent inactivation&#39;
        ]

        activator_keywords = [
            &#39;assay for activators&#39;, &#39;activation of&#39;, &#39;activators of&#39;
        ]

        inducer_keywords = [
            &#39;induction of&#39;, &#39;inducer&#39;, &#39;inducers&#39;, &#39;time-dependant induction&#39;
        ]

        # Combine all keywords into a single list for easy checking
        all_keywords = (inhibitor_keywords + ligand_keywords + inhibitor_substrate_keywords +
                        inhibitor_activator_modulator_keywords + substrate_keywords +
                        inactivator_keywords + activator_keywords + inducer_keywords)

        # Define a dictionary to map each keyword to its corresponding label
        keyword_to_label = {
            **{keyword: &#39;Inhibitor&#39; for keyword in inhibitor_keywords},
            **{keyword: &#39;Inhibitor/Substrate&#39; for keyword in inhibitor_substrate_keywords},
            **{keyword: &#39;Inhibitor/Inducer/Modulator&#39; for keyword in inhibitor_activator_modulator_keywords},
            **{keyword: &#39;Substrate&#39; for keyword in substrate_keywords},
            **{keyword: &#39;Inactivator&#39; for keyword in inactivator_keywords},
            **{keyword: &#39;Activator&#39; for keyword in activator_keywords},
            **{keyword: &#39;Inducer&#39; for keyword in inducer_keywords},
            **{keyword: &#39;Ligand&#39; for keyword in ligand_keywords},
        }

        # Function to determine the appropriate label based on the first keyword appearance in the assay name
        def determine_active_label(assay_name):
            assay_name_lower = assay_name.lower()
            first_keyword = None
            first_position = len(assay_name_lower)  # Start with the maximum length of the assay name

            for keyword in all_keywords:
                position = assay_name_lower.find(keyword)
                if 0 &lt;= position &lt; first_position:  # Check if keyword is found and is the earliest one so far
                    first_keyword = keyword
                    first_position = position

            if first_keyword:
                return keyword_to_label[first_keyword]
            return &#39;Inhibitor/Inducer/Modulator&#39;

        # Set default activity to None
        merged_df[&#39;Activity&#39;] = None

        # Handle Inactive outcomes
        inactive_mask = merged_df[&#39;Activity Outcome&#39;] == &#39;Inactive&#39;
        merged_df.loc[inactive_mask, &#39;Activity&#39;] = &#39;Inactive&#39;

        # Handle Active outcomes
        active_mask = merged_df[&#39;Activity Outcome&#39;] == &#39;Active&#39;
        if active_mask.any():
            merged_df.loc[active_mask, &#39;Activity&#39;] = merged_df.loc[active_mask, &#39;Assay Name&#39;].apply(determine_active_label)
            merged_df.loc[active_mask &amp; merged_df[&#39;Activity Name&#39;].isin([&#39;Km&#39;, &#39;Drug metabolism&#39;]), &#39;Activity&#39;] = &#39;Substrate&#39;

            # Apply the changes to the &#39;Activity&#39; column based on the combined conditions
            substrate_pattern = r&#39;(activity of.*oxidation)|(activity at cyp.*phenotyping)|(activity at human recombinant cyp.*formation)|(activity at recombinant cyp.*formation)&#39;
            merged_df.loc[active_mask &amp; merged_df[&#39;Assay Name&#39;].str.contains(substrate_pattern, case=False, regex=True), &#39;Activity&#39;] = &#39;Substrate&#39;

            ActIndMod_pattern = r&#39;(effect on cyp)|(effect on human recombinant cyp)|(effect on recombinant cyp)|(effect on human cyp)&#39;
            merged_df.loc[active_mask &amp; merged_df[&#39;Assay Name&#39;].str.contains(ActIndMod_pattern, case=False, regex=True), &#39;Activity&#39;] = &#39;Inhibitor/Inducer/Modulator&#39;

            inducer_pattern = r&#39;(effect on cyp.*induction)|(induction of.*)&#39;
            merged_df.loc[active_mask &amp; merged_df[&#39;Assay Name&#39;].str.contains(inducer_pattern, case=False, regex=True), &#39;Activity&#39;] = &#39;Inducer&#39;

            merged_df.loc[active_mask &amp; merged_df[&#39;Activity Direction&#39;].str.contains(&#39;decreasing&#39;, case=False), &#39;Activity&#39;] = &#39;Inhibitor&#39;
            merged_df.loc[active_mask &amp; merged_df[&#39;Activity Direction&#39;].str.contains(&#39;increasing&#39;, case=False), &#39;Activity&#39;] = &#39;Activator&#39;
            merged_df.loc[active_mask &amp; (merged_df[&#39;AID&#39;] == 1215398), &#39;Activity&#39;] = &#39;Inactivator&#39;

        return merged_df</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="chemgraphbuilder.RelationshipDataProcessor.most_frequent"><code class="name flex">
<span>def <span class="ident">most_frequent</span></span>(<span>row)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the most frequent string value in a row, excluding NaNs and non-string types.</p>
<p>Parameters:
row (Series): The row of data.</p>
<p>Returns:
str or None: The most frequent string value, or None if no strings are found.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipDataProcessor.propagate_phenotype"><code class="name flex">
<span>def <span class="ident">propagate_phenotype</span></span>(<span>group)</span>
</code></dt>
<dd>
<div class="desc"><p>Propagate non-null values within each group for the 'Phenotype' column.</p>
<p>Parameters:
group (DataFrame): The group of data.</p>
<p>Returns:
DataFrame: The group with propagated 'Phenotype' values.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.RelationshipDataProcessor.process_files"><code class="name flex">
<span>def <span class="ident">process_files</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Process all CSV files by reading, cleaning, merging, and formatting the data.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor"><code class="flex name class">
<span>class <span class="ident">RelationshipPropertiesExtractor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and analyzes relationship properties among compounds, genes, and assays from the PubChem database.</p>
<p>This class facilitates the retrieval of complex relational data between chemical entities, enabling detailed
analysis of biochemical interactions and properties. The extracted data is ideal for constructing knowledge
graphs, supporting drug discovery, and understanding genetic influences on compound behavior.</p>
<p>Methods within the class are tailored to query specific relationship types from PubChem, including
compound-assay relationships, compound co-occurrences, and compound transformations influenced by genes.
Data fetched from PubChem is processed and saved in structured formats (CSV files), ready for further analysis
or database integration.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>session</code></strong> :&ensp;<code>requests.Session</code></dt>
<dd>Session object to persist certain parameters across requests.</dd>
</dl>
<h2 id="usage">Usage</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
&gt;&gt;&gt; extractor.assay_compound_relationship(&quot;Data/AllDataCollected.csv&quot;)
This example fetches assay-compound relationship data for specified assays and saves the data to CSV files.
</code></pre>
<p>Initializes a RelationshipPropertiesExtractor with a Requests session for efficient network calls.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RelationshipPropertiesExtractor:
    &#34;&#34;&#34;
    Extracts and analyzes relationship properties among compounds, genes, and assays from the PubChem database.

    This class facilitates the retrieval of complex relational data between chemical entities, enabling detailed
    analysis of biochemical interactions and properties. The extracted data is ideal for constructing knowledge
    graphs, supporting drug discovery, and understanding genetic influences on compound behavior.

    Methods within the class are tailored to query specific relationship types from PubChem, including
    compound-assay relationships, compound co-occurrences, and compound transformations influenced by genes.
    Data fetched from PubChem is processed and saved in structured formats (CSV files), ready for further analysis
    or database integration.

    Attributes:
        session (requests.Session): Session object to persist certain parameters across requests.

    Usage:
        &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
        &gt;&gt;&gt; extractor.assay_compound_relationship(&#34;Data/AllDataCollected.csv&#34;)
        This example fetches assay-compound relationship data for specified assays and saves the data to CSV files.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initializes a RelationshipPropertiesExtractor with a Requests session for efficient network calls.&#34;&#34;&#34;
        self.session = requests.Session()


    def create_data_directories(self):
        &#34;&#34;&#34;
        Creates the necessary directories for storing fetched and processed data if they do not already exist.

        This method checks for the existence of several predefined directories where data will be saved during the execution
        of other methods within this class. If any of these directories do not exist, they are created. This setup ensures
        that data saving operations do not encounter errors due to missing directories.

        The directories created include:
        - &#39;Data/Relationships/Assay_Compound_Relationship&#39;: For storing relationships between assays and compounds.
        - &#39;Data/Relationships/Cpd_Cpd_CoOcuurence&#39;: For storing compound-compound co-occurrence data.
        - &#39;Data/Relationships/Cpd_gene_CoOcuurence&#39;: For storing compound-gene co-occurrence data.
        - &#39;Data/Relationships/Compound_Transformation&#39;: For storing compound transformation data.

        Side Effects:
            Creates directories on the filesystem if they do not exist.

        Example:
            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
            &gt;&gt;&gt; extractor.create_data_directories()
            This example ensures that the required directories for data storage are available before fetching and processing data.

        Note:
            This method should be called before executing data fetching and processing methods to ensure the required directories are available.
            It is designed to prevent FileNotFoundError when methods attempt to save data to these directories.
        &#34;&#34;&#34;
        directories = [
            &#39;Data/Relationships&#39;,
            &#39;Data/Relationships/Assay_Compound_Relationship&#39;,
            &#39;Data/Relationships/Cpd_Cpd_CoOcuurence&#39;,
            &#39;Data/Relationships/Cpd_gene_CoOcuurence&#39;,
            &#39;Data/Relationships/Compound_Similarities&#39;
        ]

        # Iterate through the directories list and create each one if it doesn&#39;t exist
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory)
                print(f&#34;Created directory: {directory}&#34;)
            else:
                print(f&#34;Directory already exists: {directory}&#34;)


    def _send_request(self, url, max_retries=5, initial_wait=1):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response
            except requests.HTTPError as e:
                if response.status_code == 503:
                    wait = initial_wait * (2 ** attempt)
                    print(f&#34;Server busy or under maintenance. Retrying in {wait} seconds...&#34;)
                    time.sleep(wait)
                else:
                    print(f&#34;HTTP Error: {e}&#34;)
                    break  # Break the loop for non-503 HTTP errors
            except requests.RequestException as e:
                print(f&#34;Request Exception: {e}&#34;)
                wait = initial_wait * (2 ** attempt)
                print(f&#34;Network error. Retrying in {wait} seconds...&#34;)
                time.sleep(wait)
        return None  # Return None to indicate failure after all retries


    def fetch_data_for_aid(self, aid, columns_to_remove):
        &#34;&#34;&#34;
        Fetches and processes assay data for a specified Assay ID (AID) from the PubChem database, preparing it for analysis or further processing.

        This method queries the PubChem database for assay data associated with a given AID. It constructs the query URL,
        sends the request using a previously established session, and processes the response. The response is expected to be in CSV format,
        which this method reads into a pandas DataFrame. Specific columns can be removed from this DataFrame based on the requirements
        for analysis. This allows for the customization of the fetched data, making it easier to work with specific datasets.

        If the request is successful and the data is fetched without issues, it undergoes initial processing to remove unwanted columns
        as specified by the &#39;columns_to_remove&#39; parameter. In case of an error during the data fetching or processing (e.g., issues with parsing the CSV data),
        appropriate error messages are logged, and an empty DataFrame is returned as a fallback.

        Parameters:
            aid (int): The assay ID for which data is to be fetched. This ID is used to construct the query URL to the PubChem database.
            columns_to_remove (list of str): A list of column names that should be removed from the fetched DataFrame. This allows for the exclusion
                                            of data that might not be relevant to the subsequent analysis or processing steps.

        Returns:
            pandas.DataFrame: A DataFrame containing the processed data associated with the given AID. The DataFrame will exclude columns listed
                            in &#39;columns_to_remove&#39;. If the data fetching fails or if an error occurs during processing, an empty DataFrame is returned.

        Raises:
            requests.RequestException: If an error occurs during the HTTP request to the PubChem API. This includes scenarios such as timeout issues,
                                    non-200 status codes, or network-related errors. The exception is handled internally with logging, but it&#39;s
                                    important to be aware of its possibility.
            pd.errors.ParserError: If an error occurs while parsing the CSV response from PubChem into a DataFrame. This could happen due to malformed
                                data or unexpected changes in the response format. Like with RequestException, this error is logged and results in
                                the return of an empty DataFrame.

        Example:
            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
            &gt;&gt;&gt; processed_data_df = extractor.fetch_data_for_aid(12345, [&#39;UnwantedColumn1&#39;, &#39;UnwantedColumn2&#39;])
            &gt;&gt;&gt; print(processed_data_df.head())
            This example demonstrates how to fetch and process assay data for the assay with ID 12345, removing &#39;UnwantedColumn1&#39; and &#39;UnwantedColumn2&#39;
            from the resulting DataFrame. The first few rows of the processed DataFrame are printed as an output.

        Note:
            - This method is part of a class that requires a valid session with the PubChem API. Ensure that the class is properly initialized and that
            the session is active.
            - The removal of columns is an optional step and can be customized based on the analysis needs. If no columns need to be removed, pass an
            empty list as &#39;columns_to_remove&#39;.
        &#34;&#34;&#34;
        url = f&#34;https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?query=download&amp;record_type=datatable&amp;actvty=all&amp;response_type=display&amp;aid={aid}&#34;
        response = self._send_request(url)
        if response and response.status_code == 200:
            try:
                compound_df = pd.read_csv(StringIO(response.text), sep=&#39;,&#39;)
                # Drop specified columns and process column names in-place for memory efficiency
                columns_to_remove_set = set(columns_to_remove)
                existing_columns_set = set(compound_df.columns)
                columns_to_actually_remove = list(columns_to_remove_set &amp; existing_columns_set)
                compound_df.drop(columns=columns_to_actually_remove, errors=&#39;ignore&#39;, inplace=True)
                compound_df.rename(columns=lambda x: x.replace(&#39;PUBCHEM_&#39;, &#39;&#39;) if x.startswith(&#39;PUBCHEM_&#39;) else x, inplace=True)

                # compound_df.drop(columns=[col for col in columns_to_remove if col in compound_df.columns], errors=&#39;ignore&#39;, inplace=True)
                # compound_df.columns = [col.replace(&#39;PUBCHEM_&#39;, &#39;&#39;) if col.startswith(&#39;PUBCHEM_&#39;) else col for col in compound_df.columns]
                compound_df[&#39;AID&#39;] = aid
                return compound_df
            except pd.errors.ParserError as e:
                logging.error(f&#34;CSV parsing failed for AID {aid}: {e}&#34;)
        else:
            logging.error(f&#34;Failed to fetch data for AID {aid}. Status code: {response.status_code if response else &#39;No Response&#39;}&#34;)
        return pd.DataFrame()  # Return an empty DataFrame in case of failure



    def _process_dataframe(self, df, aid, columns_to_remove):
        &#34;&#34;&#34;
        Processes the DataFrame by removing specified columns and renaming others.

        Parameters:
            df (pandas.DataFrame): The DataFrame to be processed.
            aid (int): The assay ID associated with the DataFrame.
            columns_to_remove (list of str): Columns to be removed from the DataFrame.
        &#34;&#34;&#34;
        # Drop unnecessary columns efficiently
        columns_to_remove_set = set(columns_to_remove)
        df = df.drop(columns=list(columns_to_remove_set.intersection(df.columns)), errors=&#39;ignore&#39;)

        # Efficiently rename columns that start with &#39;PUBCHEM_&#39;
        df.columns = [col.replace(&#39;PUBCHEM_&#39;, &#39;&#39;) if col.startswith(&#39;PUBCHEM_&#39;) else col for col in df.columns]
        df[&#39;AID&#39;] = aid


    def assay_compound_relationship(self, assays_data, chunk_size=100):
        &#34;&#34;&#34;
        Processes and stores relationships between assays and compounds based on assay data from PubChem.

        The method utilizes chunk processing and limited concurrent requests to minimize memory usage.

        Args:
            assays_data (str): Path to a CSV file containing assay IDs (AIDs).
            chunk_size (int): Number of rows per chunk to process from the assays_data file.
            max_workers (int): Maximum number of concurrent threads for data fetching.
        &#34;&#34;&#34;

        for chunk in pd.read_csv(assays_data, chunksize=chunk_size):
            columns_to_remove = [&#39;PUBCHEM_RESULT_TAG&#39;, &#39;PUBCHEM_SID&#39;, &#39;PUBCHEM_EXT_DATASOURCE_SMILES&#39;]
            output_dir = &#39;Data/Relationships/Assay_Compound_Relationship&#39;

            # Process each assay ID in the chunk
            for aid in chunk[&#39;AID&#39;]:
                if not os.path.exists(f&#39;{output_dir}/AID_{aid}.csv&#39;):
                    df = self.fetch_data_for_aid(aid, columns_to_remove)
                    if not df.empty:
                        if not os.path.exists(output_dir):
                            os.makedirs(output_dir)
                        df.to_csv(f&#39;{output_dir}/AID_{aid}.csv&#39;, index=False)



    def _write_to_csv(self, df, filename):
        &#34;&#34;&#34;
        Writes a DataFrame to a CSV file.
        &#34;&#34;&#34;
        df.to_csv(filename, index=False)


    def assay_enzyme_relationship(self, main_data):
        &#34;&#34;&#34;
        Extracts and saves relationships between assays and enzymes from the specified dataset.

        This method processes assay data to identify relationships between assays and their target enzymes. It selects
        relevant columns from the input data, removes duplicates to ensure unique relationships, and saves the cleaned data
        to a CSV file for further analysis or integration into knowledge graphs.

        Parameters:
            main_data (str): Path to the CSV file containing the main data. The file should include columns for &#39;AID&#39;
                            (Assay ID), &#39;Target GeneID&#39;, and &#39;Activity Name&#39;.

        Returns:
            pandas.DataFrame: A DataFrame containing the unique relationships between assays and enzymes, including the assay
                            ID, target gene ID, and activity name.

        Side Effects:
            - Writes a CSV file to &#39;Data/Assay_Enzyme_Relationship.csv&#39;, containing the processed relationships data.
        &#34;&#34;&#34;
        df = pd.read_csv(main_data)
        columns_to_select = [&#39;AID&#39;, &#39;Target GeneID&#39;, &#39;Activity Name&#39;]
        df = df[columns_to_select]
        df = df.drop_duplicates(keep=&#39;first&#39;, ignore_index=True)
        df.to_csv(f&#39;Data/Assay_Enzyme_Relationship.csv&#39;, index=False)
        return df


    def gene_enzyme_relationship(self, main_data):
        &#34;&#34;&#34;
        Extracts and saves relationships between genes and enzymes based on the provided dataset.

        This method selects relevant columns to highlight the relationships between genes and their corresponding enzymes.
        It removes duplicate entries to ensure that each relationship is represented uniquely and saves the resultant data to
        a CSV file. This facilitates easy integration of genetic data into knowledge bases or further analysis.

        Parameters:
            main_data (str): Path to the CSV file containing gene and enzyme data. Expected columns include &#39;Target GeneID&#39;
                            and &#39;Target Accession&#39;.

        Returns:
            pandas.DataFrame: A DataFrame of unique gene-enzyme relationships, including gene ID and enzyme accession numbers.

        Side Effects:
            - Writes the processed data to &#39;Data/Gene_Enzyme_Relationship.csv&#39; in a structured CSV format.
        &#34;&#34;&#34;
        df = pd.read_csv(main_data)
        columns_to_select = [&#39;Target GeneID&#39;, &#39;Target Accession&#39;]
        df = df[columns_to_select]
        df = df.drop_duplicates(keep=&#39;first&#39;, ignore_index=True)
        df.to_csv(f&#39;Data/Gene_Enzyme_Relationship.csv&#39;, index=False)
        return df


    def compound_enzyme_relationship(self, main_data):
        &#34;&#34;&#34;
        Identifies and records relationships between compounds and enzymes from the input data.

        This method focuses on extracting compound-enzyme interaction data, including activity outcomes and values. It selects
        pertinent columns, removes duplicate records, and sorts the data by Compound ID and Target Accession for clarity. The
        cleaned dataset is then saved to a CSV file, providing a structured view of how compounds interact with various enzymes,
        which can be critical for drug discovery and pharmacological research.

        Parameters:
            main_data (str): Path to the CSV file with compound and enzyme data. This file should contain columns for &#39;CID&#39;
                            (Compound ID), &#39;Target Accession&#39;, &#39;Activity Outcome&#39;, &#39;Activity Name&#39;, and &#39;Activity Value [uM]&#39;.

        Returns:
            pandas.DataFrame: A DataFrame with processed compound-enzyme relationships, sorted and cleaned for direct analysis or database insertion.

        Side Effects:
            - Saves the processed relationships data to &#39;Data/Relationships/Compound_Enzyme_Relationship.csv&#39;, facilitating easy access and integration.
        &#34;&#34;&#34;
        df = pd.read_csv(main_data)
        columns_to_select = [&#39;CID&#39;, &#39;Target Accession&#39;, &#39;Activity Outcome&#39;,
                             &#39;Activity Name&#39;, &#39;Activity Value [uM]&#39;]
        df = df[columns_to_select]
        df = df.drop_duplicates(keep=&#39;first&#39;, ignore_index=True)
        df = df.sort_values([&#39;CID&#39;, &#39;Target Accession&#39;])
        df.dropna(axis=0 , thresh=1, inplace=True) ###
        df.to_csv(f&#39;Data/Relationships/Compound_Enzyme_Relationship.csv&#39;, index=False)
        return df


    def fetch_similar_cids(self, cid):
        &#34;&#34;&#34;
        Fetches similar compound IDs (CIDs) from the PubChem database for a given compound ID (CID) using 2D similarity.

        This method queries the PubChem database to find compounds that are similar to the given CID based on 2D structural
        similarity. The similarity threshold is set to 95%, and a maximum of 100 similar CIDs are fetched. The response is
        parsed from XML format to extract the similar CIDs.

        Parameters:
            cid (int): The compound ID for which similar CIDs are to be fetched.

        Returns:
            tuple: A tuple containing the original CID and a list of similar CIDs. If an error occurs, the list of similar
            CIDs will be empty.

        Raises:
            Exception: Logs an error message with the original CID and the exception if the request to PubChem fails or
            if parsing the XML response encounters an error.

        Note:
            - The method utilizes the `requests` library for HTTP requests and `xml.etree.ElementTree` for XML parsing.
            - In case of a request failure or parsing error, the method logs the error and returns the original CID with an
            empty list, allowing the calling function to handle the exception as needed.
        &#34;&#34;&#34;
        url = (&#34;https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/&#34;
               f&#34;fastsimilarity_2d/cid/{int(cid)}/cids/XML?Threshold=95&amp;MaxRecords=100&#34;)
        try:
            response = requests.get(url)
            response.raise_for_status()
            xml_data = response.text

            # Parse XML data
            tree = ET.parse(io.StringIO(xml_data))
            root = tree.getroot()

            # Extracting CID values
            similar_cids = [element.text for element in root.findall(&#39;{http://pubchem.ncbi.nlm.nih.gov/pug_rest}CID&#39;)]
            return cid, similar_cids
        except Exception as e:
            logging.error(f&#34;Error processing CID {cid}: {e}&#34;)
            return cid, []


    def process_chunk(self, chunk):
        &#34;&#34;&#34;
        Processes a chunk of CIDs in parallel to fetch similar CIDs for each CID in the chunk.

        This method uses a ThreadPoolExecutor to send out concurrent requests for fetching similar CIDs for a list of CIDs.
        The number of worker threads is set to 5. Each CID&#39;s request is handled by `fetch_similar_cids` method.

        Parameters:
            chunk (list of int): A list of compound IDs (CIDs) to process in parallel.

        Returns:
            list of tuples: A list of tuples, each containing a CID and its corresponding list of similar CIDs.

        Side Effects:
            - Utilizes concurrent threads to speed up the fetching process.
            - May log errors if any occur during the fetching of similar CIDs for individual CIDs.
        &#34;&#34;&#34;
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(self.fetch_similar_cids, cid) for cid in chunk]
            results = [future.result() for future in as_completed(futures)]
        return results


    def compound_similarity_relationship(self, main_data):
        &#34;&#34;&#34;
        Identifies and records the similarity relationships between compounds based on a list of CIDs.

        This method reads a CSV file containing compound data, filters compounds based on specific &#39;Target GeneID&#39; values,
        and fetches similar CIDs for each compound. The compounds are processed in chunks to manage memory usage and improve
        efficiency. The results are saved into separate CSV files for each chunk.

        Parameters:
            main_data (str): Path to the CSV file containing the main compound data. This file should include at least &#39;CID&#39;
                            and &#39;Target GeneID&#39; columns.

        Side Effects:
            - Reads from a CSV file specified by `main_data`.
            - Processes compounds in chunks to efficiently handle large datasets.
            - Saves the results of similar CIDs for each chunk to separate CSV files within &#39;Data/Relationships/Compound_Similarities&#39; directory.
            - Logs information about processing and potential errors during the similarity fetching process.

        Note:
            - The method filters the main data for compounds associated with specific &#39;Target GeneID&#39; values before fetching
              similar CIDs, optimizing the process for relevant compounds only.
            - The division of CIDs into chunks and concurrent processing helps in managing large datasets and utilizes
              parallelism for faster execution.
        &#34;&#34;&#34;
        # Read main data and filter
        df = pd.read_csv(main_data)
        df = df[df[&#39;Target GeneID&#39;].isin([1576, 1544, 1557, 1559, 1565])]
        df = df.dropna(subset=[&#39;CID&#39;])
        IDs = df[&#39;CID&#39;].unique().tolist()

        # Divide the IDs into chunks of 10000
        chunk_size = 10000
        chunks = [IDs[i:i + chunk_size] for i in range(0, len(IDs), chunk_size)]

        for i, chunk in enumerate(chunks, start=0): #chuncks rannge, numbering start for naming
            chunk_results = self.process_chunk(chunk)
            chunk_df = pd.DataFrame(chunk_results, columns=[&#39;CID&#39;, &#39;Similar CIDs&#39;])
            chunk_df.to_csv(f&#39;Data/Relationships/Compound_Similarities/Chunk_{i}.csv&#39;, index=False)


    def _fetch_data(self, cid):
        &#34;&#34;&#34;
        Fetches chemical-chemical and chemical-gene relationship data for a given compound ID (CID).
        Checks if each data file exists before fetching.

        Args:
            cid (int): The compound ID for which data is to be fetched.

        Returns:
            tuple: A tuple containing the CID, and two lists of data (chemical-chemical and chemical-gene relationships).
        &#34;&#34;&#34;
        cpd_cpd_file = f&#39;Data/Relationships/Cpd_Cpd_CoOcuurence/CID_{cid}.csv&#39;
        cpd_gene_file = f&#39;Data/Relationships/Cpd_gene_CoOcuurence/CID_{cid}.csv&#39;

        cpd_cpd_data = self._fetch_chemical_neighbor_data(cid) if not os.path.exists(cpd_cpd_file) else []
        cpd_gene_data = self._fetch_chemical_gene_data(cid) if not os.path.exists(cpd_gene_file) else []

        return cid, cpd_cpd_data, cpd_gene_data


    def _fetch_chemical_neighbor_data(self, cid):
        &#34;&#34;&#34;
        Fetches chemical-chemical relationship data for a given CID.

        Args:
            cid (int): The compound ID for which data is to be fetched.

        Returns:
            list: List of chemical-chemical relationship data.
        &#34;&#34;&#34;
        cpd_cpd_url = (f&#34;https://pubchem.ncbi.nlm.nih.gov/link_db/link_db_server.cgi?format=JSON&amp;type=&#34;
                       f&#34;ChemicalNeighbor&amp;operation=GetAllLinks&amp;id_1={cid}&amp;response_type=display&#34;)
        try:
            response = self._send_request(cpd_cpd_url)
            data = response.json()
            return data.get(&#39;LinkDataSet&#39;, {}).get(&#39;LinkData&#39;, [])
        except Exception as e:
            logging.error(f&#34;Failed to fetch chemical-chemical data for CID {cid}: {e}&#34;)
            return []


    def _fetch_chemical_gene_data(self, cid):
        &#34;&#34;&#34;
        Fetches chemical-gene relationship data for a given CID.

        Args:
            cid (int): The compound ID for which data is to be fetched.

        Returns:
            list: List of chemical-gene relationship data.
        &#34;&#34;&#34;
        cpd_gene_url = (f&#34;https://pubchem.ncbi.nlm.nih.gov/link_db/link_db_server.cgi?format=JSON&amp;type=&#34;
                        f&#34;ChemicalGeneSymbolNeighbor&amp;operation=GetAllLinks&amp;id_1={cid}&amp;response_type=display&#34;)
        try:
            response = self._send_request(cpd_gene_url)
            data = response.json()
            return data.get(&#39;LinkDataSet&#39;, {}).get(&#39;LinkData&#39;, [])
        except Exception as e:
            logging.error(f&#34;Failed to fetch chemical-gene data for CID {cid}: {e}&#34;)
            return []


    def _write_data_to_csv(self, data, filename, filter_condition=None):
        &#34;&#34;&#34;
        Writes given data to a CSV file, with optional filtering before saving.

        This method takes a list of dictionaries (data), converts it into a pandas DataFrame, and optionally filters the DataFrame based on
        specified conditions before writing the result to a CSV file. The filtering is performed on specified columns with their expected
        values provided in &#39;filter_condition&#39;. This allows for selective data saving, especially useful when dealing with large datasets
        or when only a subset of data is needed for further processing or analysis.

        Parameters:
            data (list of dict): Data to be written to a CSV file. Each dictionary in the list represents a row in the DataFrame,
                                with keys as column names and values as row values.
            filename (str): Path to the CSV file where the data will be saved. If the file exists, it will be overwritten.
            filter_condition (dict, optional): A dictionary specifying the columns to filter by and the values to include. Keys in the
                                                dictionary are column names, and values are lists of acceptable values for that column.
                                                Rows not meeting the filter condition are excluded from the final DataFrame to be saved.

        Side Effects:
            - Writes a CSV file to the given filename path. The file is overwritten if it already exists.
            - Logs a warning if a specified column for filtering is not found in the DataFrame.
        &#34;&#34;&#34;

        df = pd.DataFrame(data)
        if filter_condition:
            for column, values in filter_condition.items():
                if column in df.columns:
                    df = df[df[column].isin(values)]
                else:
                    logging.warning(f&#34;Column {column} not found in DataFrame.&#34;)
        if not df.empty:
            df.to_csv(filename, index=False)


    def compound_cooccurrence(self, main_data, rate_limit=5):
        &#34;&#34;&#34;
        Analyzes compound co-occurrence relationships from the specified main data file and saves the results into structured CSV files.

        This method takes a path to a CSV file containing compound data and performs batch processing to extract relationships
        between compounds and genes from the PubChem database. It filters compounds based on their association with specific genes
        of interest, then fetches co-occurrence data for each compound using parallel requests. The data fetched includes both
        compound-compound and compound-gene co-occurrence relationships. Results are saved in separate CSV files within specific
        directories for later analysis.

        Parameters:
            main_data (str): Path to the CSV file containing the main data. This file should include &#39;CID&#39; (Compound ID) and
                            &#39;Target GeneID&#39; columns.
            rate_limit (int): Controls the rate of API requests to avoid exceeding PubChem&#39;s request limits. Specifies the maximum
                            number of requests that can be made per second.

        Side Effects:
            - Creates CSV files in &#39;Data/Relationships/Cpd_Cpd_CoOcuurence&#39; and &#39;Data/Relationships/Cpd_gene_CoOcuurence&#39;
            directories, storing compound-compound and compound-gene co-occurrence data, respectively.
            - Logs the processing time for each chunk of data and any errors encountered during data fetching.

        Returns:
            str: A message indicating the successful completion of data processing and saving.

        Raises:
            FileNotFoundError: If the specified &#39;main_data&#39; file does not exist or cannot be read.
            ValueError: If &#39;main_data&#39; does not contain the required columns (&#39;CID&#39; and &#39;Target GeneID&#39;).

        Example:
            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
            &gt;&gt;&gt; completion_message = extractor.compound_cooccurrence(&#39;Data/AllDataConnected.csv&#39;, rate_limit=5)
            &gt;&gt;&gt; print(completion_message)
            This would process the compound data, fetch co-occurrence data from PubChem, and save the results into CSV files.
            The completion message would indicate successful processing.

        Note:
            The &#39;main_data&#39; file must be properly formatted, with at least &#39;CID&#39; and &#39;Target GeneID&#39; columns present. The method
            assumes the existence of &#39;Data/Relationships/Cpd_Cpd_CoOcuurence&#39; and &#39;Data/Relationships/Cpd_gene_CoOcuurence&#39;
            directories for saving the output CSV files. It is recommended to check and adhere to PubChem&#39;s current rate limits
            when setting the &#39;rate_limit&#39; parameter to avoid potential blocks or restrictions on your IP address due to excessive requests.
        &#34;&#34;&#34;
        df = pd.read_csv(main_data, chunksize=3000)  # Reading in chunks for large files
        for chunk in df:
            chunk = chunk[chunk[&#39;Target GeneID&#39;].isin([1576, 1544, 1557, 1559, 1565])]
            chunk.dropna(subset=[&#39;CID&#39;], inplace=True)
            IDs = chunk[&#39;CID&#39;].unique().tolist()

            start_time = timeit.default_timer()
            with ThreadPoolExecutor(max_workers=rate_limit) as executor:
                futures = {executor.submit(self._fetch_data, int(cid)): cid for cid in IDs}
                for future in as_completed(futures):
                    cid, cpd_cpd_data, cpd_gene_data = future.result()
                    self._write_data_to_csv(cpd_cpd_data, f&#39;Data/Relationships/Cpd_Cpd_CoOcuurence/CID_{cid}.csv&#39;)
                    self._write_data_to_csv(cpd_gene_data, f&#39;Data/Relationships/Cpd_gene_CoOcuurence/CID_{cid}.csv&#39;,
                                            filter_condition={&#34;ID_2&#34;: [&#34;{&#39;GeneSymbol&#39;: &#39;cyp3a4&#39;}&#34;, &#34;{&#39;GeneSymbol&#39;: &#39;cyp1a2&#39;}&#34;,
                                                                    &#34;{&#39;GeneSymbol&#39;: &#39;cyp2c9&#39;}&#34;, &#34;{&#39;GeneSymbol&#39;: &#39;cyp2c19&#39;}&#34;,
                                                                    &#34;{&#39;GeneSymbol&#39;: &#39;cyp2d6&#39;}&#34;]})
                    time.sleep(1 / rate_limit)  # Ensuring we don&#39;t exceed rate limit
            elapsed = timeit.default_timer() - start_time
            logging.info(f&#34;Processed chunk in {elapsed:.2f} seconds&#34;)

        return &#34;Data fetching and saving completed.&#34;


    def compound_transformation(self, gene_properties):
        &#34;&#34;&#34;
        Analyzes compound transformation data based on gene properties, focusing on metabolic transformations
        involving specified genes. This method queries the PubChem database for transformation data related
        to compounds associated with the genes identified in the provided CSV file.

        Parameters:
            gene_properties (str): Path to the CSV file containing gene properties generated by the NodePropertiesExtractor
                                class, which should include &#39;GeneID&#39; as one of its columns. This file is used to identify
                                genes of interest for which compound transformation data will be fetched.

        Processing Steps:
            1. Reads the provided CSV file to extract unique gene identifiers.
            2. For each gene identifier, constructs a query to fetch relevant compound transformation data from
            PubChem, focusing on metabolic transformations where the gene plays a role.
            3. Processes and aggregates the fetched data into a structured pandas DataFrame.
            4. Filters the aggregated data to retain specific columns relevant to compound transformations,
            including substrate and metabolite Compound IDs (CIDs), the type of metabolic conversion, gene
            identifiers, PubMed IDs, and DOIs for related publications.
            5. Saves the aggregated and filtered DataFrame to a CSV file for further analysis or integration
            into knowledge graphs or other data models.

        Returns:
            pandas.DataFrame: A DataFrame containing processed compound transformation data, including substrate
                            and metabolite CIDs, metabolic conversion types, gene identifiers, PubMed IDs, and
                            DOIs. The DataFrame structure facilitates further analysis or use in constructing
                            detailed views of metabolic pathways involving the specified genes.

        Side Effects:
            - Saves the aggregated compound transformation data to &#39;Data/Relationships/Compound_Transformation.csv&#39;
            in the current working directory. This file captures the relationship between substrates, metabolites,
            and genes based on the input gene properties.

        Raises:
            FileNotFoundError: If the specified &#39;gene_properties&#39; file does not exist or cannot be read.
            ValueError: If &#39;gene_properties&#39; does not contain the required &#39;GeneID&#39; column.

        Example:
            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
            &gt;&gt;&gt; transformation_df = extractor.compound_transformation(&#39;Data/Nodes/gene_properties.csv&#39;)
            &gt;&gt;&gt; print(transformation_df.head())
            This example processes gene properties from &#39;path/to/gene_properties.csv&#39;, queries PubChem for
            compound transformation data related to the genes, and compiles the results into a DataFrame.

        Note:
            The method assumes that the input &#39;gene_properties&#39; file is accessible and correctly formatted.
            The availability and structure of the PubChem database may affect the completeness and accuracy
            of the fetched transformation data. Users should verify the existence of the &#39;Data/Relationships&#39;
            directory and have appropriate permissions to write files to it.
        &#34;&#34;&#34;
        df = pd.read_csv(gene_properties)
        IDs = df[&#39;GeneID&#39;].unique().tolist()

        transformation_dfs = []

        for gid in IDs:
            gid = int(gid)
            url = (f&#34;https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&amp;outfmt=csv&#34;
                   f&#34;&amp;query={{\&#34;download\&#34;:\&#34;*\&#34;,\&#34;collection\&#34;:\&#34;chemblmetabolism\&#34;,\&#34;where\&#34;:&#34;
                   f&#34;{{\&#34;ands\&#34;:[{{\&#34;geneid\&#34;:\&#34;{gid}\&#34;}}]}},\&#34;order\&#34;:[\&#34;relevancescore,desc\&#34;]&#34;
                   f&#34;,\&#34;start\&#34;:1,\&#34;limit\&#34;:10000000,\&#34;downloadfilename\&#34;:\&#34;pubchem_geneid_{gid}_chemblmetabolism\&#34;}}&#34;)

            response = self._send_request(url)
            if response:
                try:
                    transformation_df = pd.read_csv(StringIO(response.text), sep=&#39;,&#39;, header=0, low_memory=False)
                    transformation_df = transformation_df[[&#39;substratecid&#39;, &#39;metabolitecid&#39;, &#39;metconversion&#39;, &#39;geneids&#39;, &#39;pmids&#39;, &#39;dois&#39;]]
                    transformation_dfs.append(transformation_df)
                except pd.errors.ParserError as e:
                    print(f&#34;Error parsing CSV for gene ID {gid}: {e}&#34;)
                    continue  # Skip this gene ID and continue with others

        transformation_df = pd.concat(transformation_dfs, ignore_index=True) if transformation_dfs else pd.DataFrame()
        self._write_to_csv(transformation_df, &#39;Data/Relationships/Compound_Transformation.csv&#39;)

        return transformation_df</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.assay_compound_relationship"><code class="name flex">
<span>def <span class="ident">assay_compound_relationship</span></span>(<span>self, assays_data, chunk_size=100)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes and stores relationships between assays and compounds based on assay data from PubChem.</p>
<p>The method utilizes chunk processing and limited concurrent requests to minimize memory usage.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>assays_data</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to a CSV file containing assay IDs (AIDs).</dd>
<dt><strong><code>chunk_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of rows per chunk to process from the assays_data file.</dd>
<dt><strong><code>max_workers</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of concurrent threads for data fetching.</dd>
</dl></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.assay_enzyme_relationship"><code class="name flex">
<span>def <span class="ident">assay_enzyme_relationship</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and saves relationships between assays and enzymes from the specified dataset.</p>
<p>This method processes assay data to identify relationships between assays and their target enzymes. It selects
relevant columns from the input data, removes duplicates to ensure unique relationships, and saves the cleaned data
to a CSV file for further analysis or integration into knowledge graphs.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to the CSV file containing the main data. The file should include columns for 'AID'
(Assay ID), 'Target GeneID', and 'Activity Name'.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A DataFrame containing the unique relationships between assays and enzymes, including the assay
ID, target gene ID, and activity name.</dd>
</dl>
<p>Side Effects:
- Writes a CSV file to 'Data/Assay_Enzyme_Relationship.csv', containing the processed relationships data.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.compound_cooccurrence"><code class="name flex">
<span>def <span class="ident">compound_cooccurrence</span></span>(<span>self, main_data, rate_limit=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyzes compound co-occurrence relationships from the specified main data file and saves the results into structured CSV files.</p>
<p>This method takes a path to a CSV file containing compound data and performs batch processing to extract relationships
between compounds and genes from the PubChem database. It filters compounds based on their association with specific genes
of interest, then fetches co-occurrence data for each compound using parallel requests. The data fetched includes both
compound-compound and compound-gene co-occurrence relationships. Results are saved in separate CSV files within specific
directories for later analysis.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to the CSV file containing the main data. This file should include 'CID' (Compound ID) and
'Target GeneID' columns.
rate_limit (int): Controls the rate of API requests to avoid exceeding PubChem's request limits. Specifies the maximum
number of requests that can be made per second.</p>
<p>Side Effects:
- Creates CSV files in 'Data/Relationships/Cpd_Cpd_CoOcuurence' and 'Data/Relationships/Cpd_gene_CoOcuurence'
directories, storing compound-compound and compound-gene co-occurrence data, respectively.
- Logs the processing time for each chunk of data and any errors encountered during data fetching.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>A message indicating the successful completion of data processing and saving.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>FileNotFoundError</code></dt>
<dd>If the specified 'main_data' file does not exist or cannot be read.</dd>
<dt><code>ValueError</code></dt>
<dd>If 'main_data' does not contain the required columns ('CID' and 'Target GeneID').</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
&gt;&gt;&gt; completion_message = extractor.compound_cooccurrence('Data/AllDataConnected.csv', rate_limit=5)
&gt;&gt;&gt; print(completion_message)
This would process the compound data, fetch co-occurrence data from PubChem, and save the results into CSV files.
The completion message would indicate successful processing.
</code></pre>
<h2 id="note">Note</h2>
<p>The 'main_data' file must be properly formatted, with at least 'CID' and 'Target GeneID' columns present. The method
assumes the existence of 'Data/Relationships/Cpd_Cpd_CoOcuurence' and 'Data/Relationships/Cpd_gene_CoOcuurence'
directories for saving the output CSV files. It is recommended to check and adhere to PubChem's current rate limits
when setting the 'rate_limit' parameter to avoid potential blocks or restrictions on your IP address due to excessive requests.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.compound_enzyme_relationship"><code class="name flex">
<span>def <span class="ident">compound_enzyme_relationship</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Identifies and records relationships between compounds and enzymes from the input data.</p>
<p>This method focuses on extracting compound-enzyme interaction data, including activity outcomes and values. It selects
pertinent columns, removes duplicate records, and sorts the data by Compound ID and Target Accession for clarity. The
cleaned dataset is then saved to a CSV file, providing a structured view of how compounds interact with various enzymes,
which can be critical for drug discovery and pharmacological research.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to the CSV file with compound and enzyme data. This file should contain columns for 'CID'
(Compound ID), 'Target Accession', 'Activity Outcome', 'Activity Name', and 'Activity Value [uM]'.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A DataFrame with processed compound-enzyme relationships, sorted and cleaned for direct analysis or database insertion.</dd>
</dl>
<p>Side Effects:
- Saves the processed relationships data to 'Data/Relationships/Compound_Enzyme_Relationship.csv', facilitating easy access and integration.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.compound_similarity_relationship"><code class="name flex">
<span>def <span class="ident">compound_similarity_relationship</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Identifies and records the similarity relationships between compounds based on a list of CIDs.</p>
<p>This method reads a CSV file containing compound data, filters compounds based on specific 'Target GeneID' values,
and fetches similar CIDs for each compound. The compounds are processed in chunks to manage memory usage and improve
efficiency. The results are saved into separate CSV files for each chunk.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to the CSV file containing the main compound data. This file should include at least 'CID'
and 'Target GeneID' columns.</p>
<p>Side Effects:
- Reads from a CSV file specified by <code>main_data</code>.
- Processes compounds in chunks to efficiently handle large datasets.
- Saves the results of similar CIDs for each chunk to separate CSV files within 'Data/Relationships/Compound_Similarities' directory.
- Logs information about processing and potential errors during the similarity fetching process.</p>
<h2 id="note">Note</h2>
<ul>
<li>The method filters the main data for compounds associated with specific 'Target GeneID' values before fetching
similar CIDs, optimizing the process for relevant compounds only.</li>
<li>The division of CIDs into chunks and concurrent processing helps in managing large datasets and utilizes
parallelism for faster execution.</li>
</ul></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.compound_transformation"><code class="name flex">
<span>def <span class="ident">compound_transformation</span></span>(<span>self, gene_properties)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyzes compound transformation data based on gene properties, focusing on metabolic transformations
involving specified genes. This method queries the PubChem database for transformation data related
to compounds associated with the genes identified in the provided CSV file.</p>
<h2 id="parameters">Parameters</h2>
<p>gene_properties (str): Path to the CSV file containing gene properties generated by the NodePropertiesExtractor
class, which should include 'GeneID' as one of its columns. This file is used to identify
genes of interest for which compound transformation data will be fetched.</p>
<p>Processing Steps:
1. Reads the provided CSV file to extract unique gene identifiers.
2. For each gene identifier, constructs a query to fetch relevant compound transformation data from
PubChem, focusing on metabolic transformations where the gene plays a role.
3. Processes and aggregates the fetched data into a structured pandas DataFrame.
4. Filters the aggregated data to retain specific columns relevant to compound transformations,
including substrate and metabolite Compound IDs (CIDs), the type of metabolic conversion, gene
identifiers, PubMed IDs, and DOIs for related publications.
5. Saves the aggregated and filtered DataFrame to a CSV file for further analysis or integration
into knowledge graphs or other data models.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A DataFrame containing processed compound transformation data, including substrate
and metabolite CIDs, metabolic conversion types, gene identifiers, PubMed IDs, and
DOIs. The DataFrame structure facilitates further analysis or use in constructing
detailed views of metabolic pathways involving the specified genes.</dd>
</dl>
<p>Side Effects:
- Saves the aggregated compound transformation data to 'Data/Relationships/Compound_Transformation.csv'
in the current working directory. This file captures the relationship between substrates, metabolites,
and genes based on the input gene properties.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>FileNotFoundError</code></dt>
<dd>If the specified 'gene_properties' file does not exist or cannot be read.</dd>
<dt><code>ValueError</code></dt>
<dd>If 'gene_properties' does not contain the required 'GeneID' column.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
&gt;&gt;&gt; transformation_df = extractor.compound_transformation('Data/Nodes/gene_properties.csv')
&gt;&gt;&gt; print(transformation_df.head())
This example processes gene properties from 'path/to/gene_properties.csv', queries PubChem for
compound transformation data related to the genes, and compiles the results into a DataFrame.
</code></pre>
<h2 id="note">Note</h2>
<p>The method assumes that the input 'gene_properties' file is accessible and correctly formatted.
The availability and structure of the PubChem database may affect the completeness and accuracy
of the fetched transformation data. Users should verify the existence of the 'Data/Relationships'
directory and have appropriate permissions to write files to it.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.create_data_directories"><code class="name flex">
<span>def <span class="ident">create_data_directories</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the necessary directories for storing fetched and processed data if they do not already exist.</p>
<p>This method checks for the existence of several predefined directories where data will be saved during the execution
of other methods within this class. If any of these directories do not exist, they are created. This setup ensures
that data saving operations do not encounter errors due to missing directories.</p>
<p>The directories created include:
- 'Data/Relationships/Assay_Compound_Relationship': For storing relationships between assays and compounds.
- 'Data/Relationships/Cpd_Cpd_CoOcuurence': For storing compound-compound co-occurrence data.
- 'Data/Relationships/Cpd_gene_CoOcuurence': For storing compound-gene co-occurrence data.
- 'Data/Relationships/Compound_Transformation': For storing compound transformation data.</p>
<p>Side Effects:
Creates directories on the filesystem if they do not exist.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
&gt;&gt;&gt; extractor.create_data_directories()
This example ensures that the required directories for data storage are available before fetching and processing data.
</code></pre>
<h2 id="note">Note</h2>
<p>This method should be called before executing data fetching and processing methods to ensure the required directories are available.
It is designed to prevent FileNotFoundError when methods attempt to save data to these directories.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.fetch_data_for_aid"><code class="name flex">
<span>def <span class="ident">fetch_data_for_aid</span></span>(<span>self, aid, columns_to_remove)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches and processes assay data for a specified Assay ID (AID) from the PubChem database, preparing it for analysis or further processing.</p>
<p>This method queries the PubChem database for assay data associated with a given AID. It constructs the query URL,
sends the request using a previously established session, and processes the response. The response is expected to be in CSV format,
which this method reads into a pandas DataFrame. Specific columns can be removed from this DataFrame based on the requirements
for analysis. This allows for the customization of the fetched data, making it easier to work with specific datasets.</p>
<p>If the request is successful and the data is fetched without issues, it undergoes initial processing to remove unwanted columns
as specified by the 'columns_to_remove' parameter. In case of an error during the data fetching or processing (e.g., issues with parsing the CSV data),
appropriate error messages are logged, and an empty DataFrame is returned as a fallback.</p>
<h2 id="parameters">Parameters</h2>
<p>aid (int): The assay ID for which data is to be fetched. This ID is used to construct the query URL to the PubChem database.
columns_to_remove (list of str): A list of column names that should be removed from the fetched DataFrame. This allows for the exclusion
of data that might not be relevant to the subsequent analysis or processing steps.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A DataFrame containing the processed data associated with the given AID. The DataFrame will exclude columns listed
in 'columns_to_remove'. If the data fetching fails or if an error occurs during processing, an empty DataFrame is returned.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>requests.RequestException</code></dt>
<dd>If an error occurs during the HTTP request to the PubChem API. This includes scenarios such as timeout issues,
non-200 status codes, or network-related errors. The exception is handled internally with logging, but it's
important to be aware of its possibility.</dd>
<dt><code>pd.errors.ParserError</code></dt>
<dd>If an error occurs while parsing the CSV response from PubChem into a DataFrame. This could happen due to malformed
data or unexpected changes in the response format. Like with RequestException, this error is logged and results in
the return of an empty DataFrame.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()
&gt;&gt;&gt; processed_data_df = extractor.fetch_data_for_aid(12345, ['UnwantedColumn1', 'UnwantedColumn2'])
&gt;&gt;&gt; print(processed_data_df.head())
This example demonstrates how to fetch and process assay data for the assay with ID 12345, removing 'UnwantedColumn1' and 'UnwantedColumn2'
from the resulting DataFrame. The first few rows of the processed DataFrame are printed as an output.
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>This method is part of a class that requires a valid session with the PubChem API. Ensure that the class is properly initialized and that
the session is active.</li>
<li>The removal of columns is an optional step and can be customized based on the analysis needs. If no columns need to be removed, pass an
empty list as 'columns_to_remove'.</li>
</ul></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.fetch_similar_cids"><code class="name flex">
<span>def <span class="ident">fetch_similar_cids</span></span>(<span>self, cid)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches similar compound IDs (CIDs) from the PubChem database for a given compound ID (CID) using 2D similarity.</p>
<p>This method queries the PubChem database to find compounds that are similar to the given CID based on 2D structural
similarity. The similarity threshold is set to 95%, and a maximum of 100 similar CIDs are fetched. The response is
parsed from XML format to extract the similar CIDs.</p>
<h2 id="parameters">Parameters</h2>
<p>cid (int): The compound ID for which similar CIDs are to be fetched.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the original CID and a list of similar CIDs. If an error occurs, the list of similar</dd>
</dl>
<p>CIDs will be empty.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>Logs an error message with the original CID and the exception if the request to PubChem fails or</dd>
</dl>
<p>if parsing the XML response encounters an error.</p>
<h2 id="note">Note</h2>
<ul>
<li>The method utilizes the <code>requests</code> library for HTTP requests and <code>xml.etree.ElementTree</code> for XML parsing.</li>
<li>In case of a request failure or parsing error, the method logs the error and returns the original CID with an
empty list, allowing the calling function to handle the exception as needed.</li>
</ul></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.gene_enzyme_relationship"><code class="name flex">
<span>def <span class="ident">gene_enzyme_relationship</span></span>(<span>self, main_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts and saves relationships between genes and enzymes based on the provided dataset.</p>
<p>This method selects relevant columns to highlight the relationships between genes and their corresponding enzymes.
It removes duplicate entries to ensure that each relationship is represented uniquely and saves the resultant data to
a CSV file. This facilitates easy integration of genetic data into knowledge bases or further analysis.</p>
<h2 id="parameters">Parameters</h2>
<p>main_data (str): Path to the CSV file containing gene and enzyme data. Expected columns include 'Target GeneID'
and 'Target Accession'.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>A DataFrame of unique gene-enzyme relationships, including gene ID and enzyme accession numbers.</dd>
</dl>
<p>Side Effects:
- Writes the processed data to 'Data/Gene_Enzyme_Relationship.csv' in a structured CSV format.</p></div>
</dd>
<dt id="chemgraphbuilder.RelationshipPropertiesExtractor.process_chunk"><code class="name flex">
<span>def <span class="ident">process_chunk</span></span>(<span>self, chunk)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes a chunk of CIDs in parallel to fetch similar CIDs for each CID in the chunk.</p>
<p>This method uses a ThreadPoolExecutor to send out concurrent requests for fetching similar CIDs for a list of CIDs.
The number of worker threads is set to 5. Each CID's request is handled by <code>fetch_similar_cids</code> method.</p>
<h2 id="parameters">Parameters</h2>
<p>chunk (list of int): A list of compound IDs (CIDs) to process in parallel.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code> of <code>tuples</code></dt>
<dd>A list of tuples, each containing a CID and its corresponding list of similar CIDs.</dd>
</dl>
<p>Side Effects:
- Utilizes concurrent threads to speed up the fetching process.
- May log errors if any occur during the fetching of similar CIDs for individual CIDs.</p></div>
</dd>
</dl>
</dd>
<dt id="chemgraphbuilder.SetupDataFolder"><code class="flex name class">
<span>class <span class="ident">SetupDataFolder</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class to set up a data directory with a predefined structure.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>data_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the data folder.</dd>
<dt><strong><code>base_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The base path for the data directory.</dd>
<dt><strong><code>structure</code></strong> :&ensp;<code>dict</code></dt>
<dd>The structure of directories to create.</dd>
</dl>
<p>Initializes the DataFolderSetup with the data folder name and directory structure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SetupDataFolder:
    &#34;&#34;&#34;
    Class to set up a data directory with a predefined structure.

    Attributes:
        data_folder (str): The name of the data folder.
        base_path (str): The base path for the data directory.
        structure (dict): The structure of directories to create.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;
        Initializes the DataFolderSetup with the data folder name and directory structure.
        &#34;&#34;&#34;
        self.data_folder = &#34;Data&#34;
        self.base_path = os.path.join(os.getcwd(), self.data_folder)
        self.structure = {
            &#34;Nodes&#34;: [&#34;Compound_properties&#34;],
            &#34;Relationships&#34;: [
                &#34;Assay_Compound_Relationship&#34;,
                &#34;Compound_Similarities&#34;,
                &#34;Cpd_Cpd_CoOccurrence&#34;,
                &#34;Cpd_Gene_CoOccurrence&#34;
            ]
        }

    @staticmethod
    def create_folder(path):
        &#34;&#34;&#34;
        Creates a folder if it does not already exist.

        Args:
            path (str): The path of the folder to create.
        &#34;&#34;&#34;
        if not os.path.exists(path):
            os.makedirs(path)
            print(f&#34;Created folder: {path}&#34;)
        else:
            print(f&#34;Folder already exists: {path}&#34;)

    def setup(self):
        &#34;&#34;&#34;
        Sets up the data directory structure based on the predefined structure.
        &#34;&#34;&#34;
        # Create the base data directory
        self.create_folder(self.base_path)

        # Create the &#39;Nodes&#39; directory and its subdirectories
        nodes_path = os.path.join(self.base_path, &#34;Nodes&#34;)
        self.create_folder(nodes_path)
        for folder in self.structure[&#34;Nodes&#34;]:
            self.create_folder(os.path.join(nodes_path, folder))

        # Create the &#39;Relationships&#39; directory and its subdirectories
        relationships_path = os.path.join(self.base_path, &#34;Relationships&#34;)
        self.create_folder(relationships_path)
        for folder in self.structure[&#34;Relationships&#34;]:
            self.create_folder(os.path.join(relationships_path, folder))

        # Change the current working directory to the base data directory
        os.chdir(self.base_path)
        print(f&#34;Changed current directory to: {self.base_path}&#34;)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="chemgraphbuilder.SetupDataFolder.create_folder"><code class="name flex">
<span>def <span class="ident">create_folder</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a folder if it does not already exist.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path of the folder to create.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="chemgraphbuilder.SetupDataFolder.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the data directory structure based on the predefined structure.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="chemgraphbuilder.add_graph_nodes" href="add_graph_nodes.html">chemgraphbuilder.add_graph_nodes</a></code></li>
<li><code><a title="chemgraphbuilder.add_graph_relationships" href="add_graph_relationships.html">chemgraphbuilder.add_graph_relationships</a></code></li>
<li><code><a title="chemgraphbuilder.graph_nodes_loader" href="graph_nodes_loader.html">chemgraphbuilder.graph_nodes_loader</a></code></li>
<li><code><a title="chemgraphbuilder.graph_relationships_loader" href="graph_relationships_loader.html">chemgraphbuilder.graph_relationships_loader</a></code></li>
<li><code><a title="chemgraphbuilder.neo4jdriver" href="neo4jdriver.html">chemgraphbuilder.neo4jdriver</a></code></li>
<li><code><a title="chemgraphbuilder.node_collector_processor" href="node_collector_processor.html">chemgraphbuilder.node_collector_processor</a></code></li>
<li><code><a title="chemgraphbuilder.node_data_processor" href="node_data_processor.html">chemgraphbuilder.node_data_processor</a></code></li>
<li><code><a title="chemgraphbuilder.node_properties_extractor" href="node_properties_extractor.html">chemgraphbuilder.node_properties_extractor</a></code></li>
<li><code><a title="chemgraphbuilder.relationship_collector_processor" href="relationship_collector_processor.html">chemgraphbuilder.relationship_collector_processor</a></code></li>
<li><code><a title="chemgraphbuilder.relationship_data_processor" href="relationship_data_processor.html">chemgraphbuilder.relationship_data_processor</a></code></li>
<li><code><a title="chemgraphbuilder.relationship_properties_extractor" href="relationship_properties_extractor.html">chemgraphbuilder.relationship_properties_extractor</a></code></li>
<li><code><a title="chemgraphbuilder.setup_data_folder" href="setup_data_folder.html">chemgraphbuilder.setup_data_folder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="chemgraphbuilder.AddGraphNodes" href="#chemgraphbuilder.AddGraphNodes">AddGraphNodes</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.AddGraphNodes.combine_csv_files" href="#chemgraphbuilder.AddGraphNodes.combine_csv_files">combine_csv_files</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.create_uniqueness_constraint" href="#chemgraphbuilder.AddGraphNodes.create_uniqueness_constraint">create_uniqueness_constraint</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.execute_queries" href="#chemgraphbuilder.AddGraphNodes.execute_queries">execute_queries</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.generate_cypher_queries" href="#chemgraphbuilder.AddGraphNodes.generate_cypher_queries">generate_cypher_queries</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.process_and_add_nodes" href="#chemgraphbuilder.AddGraphNodes.process_and_add_nodes">process_and_add_nodes</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.process_and_add_nodes_from_directory" href="#chemgraphbuilder.AddGraphNodes.process_and_add_nodes_from_directory">process_and_add_nodes_from_directory</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.public_generate_property_string" href="#chemgraphbuilder.AddGraphNodes.public_generate_property_string">public_generate_property_string</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphNodes.read_csv_file" href="#chemgraphbuilder.AddGraphNodes.read_csv_file">read_csv_file</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.AddGraphRelationships" href="#chemgraphbuilder.AddGraphRelationships">AddGraphRelationships</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.AddGraphRelationships.combine_csv_files" href="#chemgraphbuilder.AddGraphRelationships.combine_csv_files">combine_csv_files</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphRelationships.execute_queries" href="#chemgraphbuilder.AddGraphRelationships.execute_queries">execute_queries</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_directories" href="#chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_directories">generate_cypher_queries_from_directories</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_file" href="#chemgraphbuilder.AddGraphRelationships.generate_cypher_queries_from_file">generate_cypher_queries_from_file</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphRelationships.process_and_add_relationships" href="#chemgraphbuilder.AddGraphRelationships.process_and_add_relationships">process_and_add_relationships</a></code></li>
<li><code><a title="chemgraphbuilder.AddGraphRelationships.process_and_add_relationships_from_directory" href="#chemgraphbuilder.AddGraphRelationships.process_and_add_relationships_from_directory">process_and_add_relationships_from_directory</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.GraphNodesLoader" href="#chemgraphbuilder.GraphNodesLoader">GraphNodesLoader</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.GraphNodesLoader.create_uniqueness_constraint" href="#chemgraphbuilder.GraphNodesLoader.create_uniqueness_constraint">create_uniqueness_constraint</a></code></li>
<li><code><a title="chemgraphbuilder.GraphNodesLoader.load_data_for_node_type" href="#chemgraphbuilder.GraphNodesLoader.load_data_for_node_type">load_data_for_node_type</a></code></li>
<li><code><a title="chemgraphbuilder.GraphNodesLoader.process_and_add_nodes" href="#chemgraphbuilder.GraphNodesLoader.process_and_add_nodes">process_and_add_nodes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.GraphRelationshipsLoader" href="#chemgraphbuilder.GraphRelationshipsLoader">GraphRelationshipsLoader</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.GraphRelationshipsLoader.add_relationships" href="#chemgraphbuilder.GraphRelationshipsLoader.add_relationships">add_relationships</a></code></li>
<li><code><a title="chemgraphbuilder.GraphRelationshipsLoader.close" href="#chemgraphbuilder.GraphRelationshipsLoader.close">close</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.Neo4jBase" href="#chemgraphbuilder.Neo4jBase">Neo4jBase</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.Neo4jBase.close" href="#chemgraphbuilder.Neo4jBase.close">close</a></code></li>
<li><code><a title="chemgraphbuilder.Neo4jBase.connect_to_neo4j" href="#chemgraphbuilder.Neo4jBase.connect_to_neo4j">connect_to_neo4j</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.NodeCollectorProcessor" href="#chemgraphbuilder.NodeCollectorProcessor">NodeCollectorProcessor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.NodeCollectorProcessor.close" href="#chemgraphbuilder.NodeCollectorProcessor.close">close</a></code></li>
<li><code><a title="chemgraphbuilder.NodeCollectorProcessor.collect_and_process_data" href="#chemgraphbuilder.NodeCollectorProcessor.collect_and_process_data">collect_and_process_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.NodeDataProcessor" href="#chemgraphbuilder.NodeDataProcessor">NodeDataProcessor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.NodeDataProcessor.preprocess_assays" href="#chemgraphbuilder.NodeDataProcessor.preprocess_assays">preprocess_assays</a></code></li>
<li><code><a title="chemgraphbuilder.NodeDataProcessor.preprocess_compounds" href="#chemgraphbuilder.NodeDataProcessor.preprocess_compounds">preprocess_compounds</a></code></li>
<li><code><a title="chemgraphbuilder.NodeDataProcessor.preprocess_genes" href="#chemgraphbuilder.NodeDataProcessor.preprocess_genes">preprocess_genes</a></code></li>
<li><code><a title="chemgraphbuilder.NodeDataProcessor.preprocess_proteins" href="#chemgraphbuilder.NodeDataProcessor.preprocess_proteins">preprocess_proteins</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.NodePropertiesExtractor" href="#chemgraphbuilder.NodePropertiesExtractor">NodePropertiesExtractor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.extract_assay_properties" href="#chemgraphbuilder.NodePropertiesExtractor.extract_assay_properties">extract_assay_properties</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.extract_compound_properties" href="#chemgraphbuilder.NodePropertiesExtractor.extract_compound_properties">extract_compound_properties</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.extract_gene_properties" href="#chemgraphbuilder.NodePropertiesExtractor.extract_gene_properties">extract_gene_properties</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.extract_protein_properties" href="#chemgraphbuilder.NodePropertiesExtractor.extract_protein_properties">extract_protein_properties</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.fetch_data" href="#chemgraphbuilder.NodePropertiesExtractor.fetch_data">fetch_data</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.get_enzyme_assays" href="#chemgraphbuilder.NodePropertiesExtractor.get_enzyme_assays">get_enzyme_assays</a></code></li>
<li><code><a title="chemgraphbuilder.NodePropertiesExtractor.run" href="#chemgraphbuilder.NodePropertiesExtractor.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.RelationshipCollectorProcessor" href="#chemgraphbuilder.RelationshipCollectorProcessor">RelationshipCollectorProcessor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.RelationshipCollectorProcessor.collect_relationship_data" href="#chemgraphbuilder.RelationshipCollectorProcessor.collect_relationship_data">collect_relationship_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.RelationshipDataProcessor" href="#chemgraphbuilder.RelationshipDataProcessor">RelationshipDataProcessor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.RelationshipDataProcessor.most_frequent" href="#chemgraphbuilder.RelationshipDataProcessor.most_frequent">most_frequent</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipDataProcessor.process_files" href="#chemgraphbuilder.RelationshipDataProcessor.process_files">process_files</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipDataProcessor.propagate_phenotype" href="#chemgraphbuilder.RelationshipDataProcessor.propagate_phenotype">propagate_phenotype</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor" href="#chemgraphbuilder.RelationshipPropertiesExtractor">RelationshipPropertiesExtractor</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.assay_compound_relationship" href="#chemgraphbuilder.RelationshipPropertiesExtractor.assay_compound_relationship">assay_compound_relationship</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.assay_enzyme_relationship" href="#chemgraphbuilder.RelationshipPropertiesExtractor.assay_enzyme_relationship">assay_enzyme_relationship</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.compound_cooccurrence" href="#chemgraphbuilder.RelationshipPropertiesExtractor.compound_cooccurrence">compound_cooccurrence</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.compound_enzyme_relationship" href="#chemgraphbuilder.RelationshipPropertiesExtractor.compound_enzyme_relationship">compound_enzyme_relationship</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.compound_similarity_relationship" href="#chemgraphbuilder.RelationshipPropertiesExtractor.compound_similarity_relationship">compound_similarity_relationship</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.compound_transformation" href="#chemgraphbuilder.RelationshipPropertiesExtractor.compound_transformation">compound_transformation</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.create_data_directories" href="#chemgraphbuilder.RelationshipPropertiesExtractor.create_data_directories">create_data_directories</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.fetch_data_for_aid" href="#chemgraphbuilder.RelationshipPropertiesExtractor.fetch_data_for_aid">fetch_data_for_aid</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.fetch_similar_cids" href="#chemgraphbuilder.RelationshipPropertiesExtractor.fetch_similar_cids">fetch_similar_cids</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.gene_enzyme_relationship" href="#chemgraphbuilder.RelationshipPropertiesExtractor.gene_enzyme_relationship">gene_enzyme_relationship</a></code></li>
<li><code><a title="chemgraphbuilder.RelationshipPropertiesExtractor.process_chunk" href="#chemgraphbuilder.RelationshipPropertiesExtractor.process_chunk">process_chunk</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="chemgraphbuilder.SetupDataFolder" href="#chemgraphbuilder.SetupDataFolder">SetupDataFolder</a></code></h4>
<ul class="">
<li><code><a title="chemgraphbuilder.SetupDataFolder.create_folder" href="#chemgraphbuilder.SetupDataFolder.create_folder">create_folder</a></code></li>
<li><code><a title="chemgraphbuilder.SetupDataFolder.setup" href="#chemgraphbuilder.SetupDataFolder.setup">setup</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
